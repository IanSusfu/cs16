{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c91eb04",
   "metadata": {},
   "source": [
    "# DPA Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2a28f2a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter '1. single' for single dataset or '2. multi' for multiple dataset: 2\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "\n",
    "sys.path.append('./cs16')\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import (Activation, Attention, Bidirectional, Concatenate, Conv1D,\n",
    "                           Dense, Dropout, Embedding, Flatten, GlobalMaxPooling1D,\n",
    "                           Input, Layer, LSTM, MaxPooling1D, Multiply, Permute,\n",
    "                           RepeatVector, Reshape, SpatialDropout1D, TimeDistributed)\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.corpus import stopwords\n",
    "from keras import regularizers\n",
    "\n",
    "import cs16.prep as prep16\n",
    "import cs16.plot as plot16\n",
    "import cs16.build as build16\n",
    "imagesize = 64\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "data_type = input(\"Enter '1. single' for single dataset or '2. multi' for multiple dataset: \")\n",
    "\n",
    "if data_type == '1':\n",
    "    file_path = 'single.txt'\n",
    "    folder_path = './data/MVSA/single/'\n",
    "elif data_type == '2':\n",
    "    file_path = 'multi.txt'\n",
    "    folder_path = './data/MVSA/multiple/'\n",
    "else:\n",
    "    print(\"Invalid input. Please enter either 'single' or 'multi'.\")\n",
    "    exit()\n",
    "\n",
    "df = pd.read_csv(file_path, index_col=None, encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "49123e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ausco\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ausco\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# Download stopwords and punkt tokenizer if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Define a function to preprocess text\n",
    "def nlp_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs using regex\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    # Remove punctuation using regex\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Tokenize text into individual words\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Stem words using Porter Stemmer\n",
    "    stemmer = PorterStemmer()\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    \n",
    "    # Join words back into a single string\n",
    "    text = ' '.join(words)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply the preprocess_text function to the 'tweet' column of the dataframe\n",
    "df['tweet'] = df['tweet'].apply(nlp_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3e5e4873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset processed is multi.txt\n"
     ]
    }
   ],
   "source": [
    "X_text, y_text = prep16.preprocess_text(df)\n",
    "X_train_text, X_val_text, X_test_text, \\\n",
    "y_train_text, y_val_text, y_test_text = prep16.split_data(X_text, y_text, random_state=42)\n",
    "\n",
    "X_polar, y_polar = prep16.preprocess_text(df,label = 'polarity')\n",
    "X_train_polar, X_val_polar, X_test_polar, \\\n",
    "y_train_polar, y_val_polar, y_test_polar = prep16.split_data(X_polar, y_polar, random_state=42)\n",
    "\n",
    "#image_data_s, image_label_s = prep16.preprocess_images(df, folder_path, imagesize)\n",
    "#y_s = to_categorical(image_label_s, num_classes=3)\n",
    "\n",
    "#X_train_image, X_val_image, X_test_image, \\\n",
    "#y_train_image, y_val_image, y_test_image= prep16.split_data(image_data_s, y_s, random_state=42)\n",
    "\n",
    "# text only......\n",
    "y_train = to_categorical(y_train_text, num_classes=3)\n",
    "y_val =to_categorical(y_val_text, num_classes=3)\n",
    "y_test =to_categorical(y_test_text, num_classes=3)\n",
    "print(f\"dataset processed is {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a89a4a",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b6fa4bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import LSTM\n",
    "#=============================================\n",
    "# LSTM\n",
    "#=============================================\n",
    "# Text model\n",
    "text_af = 'tanh'\n",
    "text_input = Input(shape=(100,), name='text_input')\n",
    "lstm1 = Embedding(input_dim=10000, output_dim=32)(text_input)\n",
    "lstm1 = Dropout(0.3)(lstm1)  # Add a dropout layer\n",
    "lstm1 = LSTM(128, activation=text_af)(lstm1)\n",
    "lstm1 = Dense(64, activation=text_af, kernel_regularizer=regularizers.l2(0.01))(lstm1)\n",
    "lstm1 = Dropout(0.5)(lstm1)  # Add a dropout layer\n",
    "text_output = Dense(3, activation='sigmoid', kernel_regularizer=regularizers.l2(0.01))(lstm1)\n",
    "textclassifier = 'LSTM'\n",
    "print(f\"{textclassifier}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83c0cca",
   "metadata": {},
   "source": [
    "# LSTM + ATT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1d238806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM +ATT\n"
     ]
    }
   ],
   "source": [
    "class ContentAttention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(ContentAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1), initializer=\"normal\")\n",
    "        self.b = self.add_weight(name=\"att_bias\", shape=(input_shape[1], 1), initializer=\"zeros\")\n",
    "        super(ContentAttention, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        e = K.tanh(K.dot(x, self.W) + self.b)\n",
    "        a = K.softmax(e, axis=1)\n",
    "        output = x * a\n",
    "        return K.sum(output, axis=1) \n",
    "from keras.layers import LSTM\n",
    "#=============================================\n",
    "# LSTM\n",
    "#=============================================\n",
    "# Text model\n",
    "text_af = 'tanh'\n",
    "text_input = Input(shape=(100,), name='text_input')\n",
    "lstm1 = Embedding(input_dim=10000, output_dim=32)(text_input)\n",
    "lstm1 = Dropout(0.3)(lstm1)  # Add a dropout layer\n",
    "lstm1 = LSTM(256, activation=text_af)(lstm1)\n",
    "lstm1 = Reshape((1, 256))(lstm1)\n",
    "lstm1 = ContentAttention()(lstm1)\n",
    "lstm1 = Dense(64, activation=text_af, kernel_regularizer=regularizers.l2(0.01))(lstm1)\n",
    "lstm1 = Dropout(0.5)(lstm1)  # Add a dropout layer\n",
    "text_output = Dense(3, activation='sigmoid', kernel_regularizer=regularizers.l2(0.01))(lstm1)\n",
    "textclassifier = 'LSTM +ATT'\n",
    "print(f\"{textclassifier}\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c1f851",
   "metadata": {},
   "source": [
    "# BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2277a981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bi-LSTM\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import LSTM\n",
    "#=============================================\n",
    "# LSTM\n",
    "#=============================================\n",
    "# Text model\n",
    "text_af = 'tanh'\n",
    "text_input = Input(shape=(100,), name='text_input')\n",
    "t = Embedding(input_dim=10000, output_dim=32)(text_input)\n",
    "t = Dropout(0.3)(t)  # Add a dropout layer\n",
    "t = Bidirectional(LSTM(128, activation=text_af))(t)\n",
    "t = Dense(64, activation=text_af, kernel_regularizer=regularizers.l2(0.01))(t)\n",
    "t = Dropout(0.5)(t)  # Add a dropout layer\n",
    "text_output = Dense(3, activation='sigmoid', kernel_regularizer=regularizers.l2(0.01))(t)\n",
    "textclassifier = 'Bi-LSTM'\n",
    "print(f\"{textclassifier}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4eee61e",
   "metadata": {},
   "source": [
    "# DUAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "aa15ccea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dual LSTM\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import LSTM\n",
    "#=============================================\n",
    "# LSTM\n",
    "#=============================================\n",
    "# Text model\n",
    "text_af = 'tanh'\n",
    "text_input = Input(shape=(100,), name='text_input')\n",
    "\n",
    "#----------LSTM1----------------\n",
    "lstm1 = Embedding(input_dim=10000, output_dim=32)(text_input)\n",
    "lstm1 = Dropout(0.3)(lstm1)  # Add a dropout layer\n",
    "lstm1 = LSTM(128, activation=text_af)(lstm1)\n",
    "lstm1 = Dense(64, activation=text_af, kernel_regularizer=regularizers.l2(0.03))(lstm1)\n",
    "lstm1 = Dropout(0.5)(lstm1)  # Add a dropout layer\n",
    "\n",
    "#---------LSTM2-----------------\n",
    "lstm2 = Embedding(input_dim=10000, output_dim=32)(text_input)\n",
    "lstm2 = Dropout(0.3)(lstm2)  # Add a dropout layer\n",
    "lstm2 = LSTM(64, activation=text_af)(lstm2)\n",
    "lstm2 = Dense(32, activation=text_af, kernel_regularizer=regularizers.l2(0.01))(lstm2)\n",
    "lstm2 = Dropout(0.5)(lstm2)  # Add a dropout layer\n",
    "\n",
    "#----------local fusion---------------\n",
    "merged_layer = Concatenate()([lstm1, lstm2]) \n",
    "merged = Dense(3, activation='softmax')(merged_layer)\n",
    "#merged = Reshape((1, 128))(merged)  \n",
    "#merged = Bidirectional(LSTM(64))(merged)\n",
    "#text_output = Dense(3, activation='sigmoid', kernel_regularizer=regularizers.l2(0.01))(merged)\n",
    "text_output = merged\n",
    "textclassifier = 'Dual LSTM'\n",
    "print(f\"{textclassifier}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e4f362",
   "metadata": {},
   "source": [
    "# Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "68269d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def print_classification_report(model, X_test, y_test):\n",
    "    # Generate the predicted labels\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Convert the predicted probabilities to class labels\n",
    "    y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "    y_true_labels = np.argmax(y_test, axis=1) \n",
    "\n",
    "    # Generate the classification report\n",
    "    report = classification_report(y_true_labels, y_pred_labels, digits=4)\n",
    "    print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f85d29",
   "metadata": {
    "tags": [
     "RUN"
    ]
   },
   "source": [
    "# RUN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c3ba695d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "122/122 [==============================] - 139s 1s/step - loss: 1.2219 - accuracy: 0.7115 - val_loss: 0.8125 - val_accuracy: 0.7360\n",
      "Epoch 2/10\n",
      "122/122 [==============================] - 146s 1s/step - loss: 0.7742 - accuracy: 0.7169 - val_loss: 0.7044 - val_accuracy: 0.7381\n",
      "Epoch 3/10\n",
      "122/122 [==============================] - 145s 1s/step - loss: 0.6638 - accuracy: 0.7364 - val_loss: 0.6996 - val_accuracy: 0.7365\n",
      "Epoch 4/10\n",
      "122/122 [==============================] - 151s 1s/step - loss: 0.6080 - accuracy: 0.7569 - val_loss: 0.7218 - val_accuracy: 0.7206\n",
      "Epoch 5/10\n",
      "122/122 [==============================] - 165s 1s/step - loss: 0.5705 - accuracy: 0.7749 - val_loss: 0.7554 - val_accuracy: 0.7149\n",
      "Epoch 6/10\n",
      "122/122 [==============================] - 169s 1s/step - loss: 0.5362 - accuracy: 0.7887 - val_loss: 0.8169 - val_accuracy: 0.7134\n",
      "Epoch 7/10\n",
      "122/122 [==============================] - 169s 1s/step - loss: 0.5062 - accuracy: 0.8047 - val_loss: 0.8226 - val_accuracy: 0.7072\n",
      "Epoch 8/10\n",
      "122/122 [==============================] - 167s 1s/step - loss: 0.4790 - accuracy: 0.8177 - val_loss: 0.8955 - val_accuracy: 0.6913\n",
      "Epoch 9/10\n",
      "122/122 [==============================] - 165s 1s/step - loss: 0.4575 - accuracy: 0.8252 - val_loss: 0.9842 - val_accuracy: 0.6944\n",
      "Epoch 10/10\n",
      "122/122 [==============================] - 173s 1s/step - loss: 0.4372 - accuracy: 0.8395 - val_loss: 1.0130 - val_accuracy: 0.6723\n",
      "61/61 [==============================] - 7s 122ms/step - loss: 1.0827 - accuracy: 0.6367\n",
      " Bi-LSTM Test accuracy:0.6366906762123108\n",
      "61/61 [==============================] - 8s 123ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.2216    0.1984    0.2093       373\n",
      "           1     0.7570    0.8186    0.7866      1378\n",
      "           2     0.3033    0.1897    0.2334       195\n",
      "\n",
      "    accuracy                         0.6367      1946\n",
      "   macro avg     0.4273    0.4022    0.4098      1946\n",
      "weighted avg     0.6089    0.6367    0.6205      1946\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# Define the model\n",
    "model = Model(inputs=text_input, outputs=text_output)\n",
    "# Compile the model\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_text, y_train, validation_data=(X_val_text, y_val), epochs=10, batch_size=128)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(X_test_text, y_test)\n",
    "print(f' {textclassifier} Test accuracy:{test_acc}')\n",
    "print_classification_report(model, X_test_text, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1fa6dbe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy of >>>>>>>>>Dual LSTM>>>>>>>>>>>>>\n",
      "61/61 [==============================] - 4s 71ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.2683    0.1769    0.2132       373\n",
      "           1     0.7592    0.8534    0.8036      1378\n",
      "           2     0.3245    0.2513    0.2832       195\n",
      "\n",
      "    accuracy                         0.6634      1946\n",
      "   macro avg     0.4507    0.4272    0.4333      1946\n",
      "weighted avg     0.6215    0.6634    0.6383      1946\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Duplicate a copy\n",
    "print(f\"Copy of >>>>>>>>>{textclassifier}>>>>>>>>>>>>>\")\n",
    "print_classification_report(model, X_test_text, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "806331f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy of >>>>>>>>>LSTM +ATT>>>>>>>>>>>>>\n",
      "61/61 [==============================] - 8s 128ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.2467    0.1984    0.2199       373\n",
      "           1     0.7568    0.8628    0.8064      1378\n",
      "           2     0.3467    0.1333    0.1926       195\n",
      "\n",
      "    accuracy                         0.6624      1946\n",
      "   macro avg     0.4501    0.3982    0.4063      1946\n",
      "weighted avg     0.6180    0.6624    0.6325      1946\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Duplicate a copy\n",
    "print(f\"Copy of >>>>>>>>>{textclassifier}>>>>>>>>>>>>>\")\n",
    "print_classification_report(model, X_test_text, y_test)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
