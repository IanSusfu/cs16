{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_data(file_path):\n",
    "    X_data = []\n",
    "    Y_data = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            example = json.loads(line.strip())\n",
    "            X_data.append(example['text'])\n",
    "            Y_data.append(example['label'])\n",
    "    return X_data, Y_data\n",
    "\n",
    "X_train, Y_train = load_data('../mgnns/train_all_anno.json')\n",
    "X_test, Y_test = load_data('../mgnns/test_all_anno.json')\n",
    "X_val, Y_val = load_data('../mgnns/val_all_anno.json')\n",
    "\n",
    "X_train = [x.lower() for x in X_train]\n",
    "X_test = [x.lower() for x in X_test]\n",
    "X_val = [x.lower() for x in X_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# Load the GLOVE embeddings\n",
    "embedding_path = '../mgnns/glove.6B.300d.txt'\n",
    "embedding_index = {}\n",
    "with open(embedding_path, encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embedding_index[word] = coefs\n",
    "\n",
    "# Define the tokenizer and fit on the text data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Convert the text to sequences of integers and pad to a length of 100\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_train_padded = pad_sequences(X_train_sequences, maxlen=100)\n",
    "\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "X_test_padded = pad_sequences(X_test_sequences, maxlen=100)\n",
    "\n",
    "X_val_sequences = tokenizer.texts_to_sequences(X_val)\n",
    "X_val_padded = pad_sequences(X_val_sequences, maxlen=100)\n",
    "\n",
    "# Create an embedding matrix for the words in the tokenizer\n",
    "word_index = tokenizer.word_index\n",
    "embedding_dim = 300\n",
    "num_words = len(word_index) + 1\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        embedding_matrix[i] = np.random.normal(size=(embedding_dim,))\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "lb = LabelBinarizer()\n",
    "Y_train_encoded = lb.fit_transform(Y_train)\n",
    "Y_val_encoded = lb.transform(Y_val)\n",
    "Y_test_encoded = lb.transform(Y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def f1_score(y_true, y_pred): \n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_4 (Embedding)     (None, 100, 300)          2571000   \n",
      "                                                                 \n",
      " lstm_8 (LSTM)               (None, 100, 128)          219648    \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 100, 128)          0         \n",
      "                                                                 \n",
      " lstm_9 (LSTM)               (None, 100, 64)           49408     \n",
      "                                                                 \n",
      " global_average_pooling1d_4   (None, 64)               0         \n",
      " (GlobalAveragePooling1D)                                        \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,844,411\n",
      "Trainable params: 273,411\n",
      "Non-trainable params: 2,571,000\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "69/69 [==============================] - ETA: 0s - loss: 1.0382 - accuracy: 0.4608 - f1_score: 0.1241\n",
      "Epoch 1: val_f1_score improved from -inf to 0.00000, saving model to lstm_avg_single.h5\n",
      "69/69 [==============================] - 14s 152ms/step - loss: 1.0382 - accuracy: 0.4608 - f1_score: 0.1241 - val_loss: 1.0207 - val_accuracy: 0.4552 - val_f1_score: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 2/30\n",
      "69/69 [==============================] - ETA: 0s - loss: 1.0174 - accuracy: 0.4585 - f1_score: 0.1702\n",
      "Epoch 2: val_f1_score improved from 0.00000 to 0.00673, saving model to lstm_avg_single.h5\n",
      "69/69 [==============================] - 12s 173ms/step - loss: 1.0174 - accuracy: 0.4585 - f1_score: 0.1702 - val_loss: 1.0054 - val_accuracy: 0.4552 - val_f1_score: 0.0067 - lr: 0.0010\n",
      "Epoch 3/30\n",
      "69/69 [==============================] - ETA: 0s - loss: 1.0005 - accuracy: 0.4798 - f1_score: 0.2170\n",
      "Epoch 3: val_f1_score improved from 0.00673 to 0.06444, saving model to lstm_avg_single.h5\n",
      "69/69 [==============================] - 12s 167ms/step - loss: 1.0005 - accuracy: 0.4798 - f1_score: 0.2170 - val_loss: 0.9870 - val_accuracy: 0.4813 - val_f1_score: 0.0644 - lr: 0.0010\n",
      "Epoch 4/30\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.9688 - accuracy: 0.5165 - f1_score: 0.2800\n",
      "Epoch 4: val_f1_score improved from 0.06444 to 0.24469, saving model to lstm_avg_single.h5\n",
      "69/69 [==============================] - 12s 167ms/step - loss: 0.9688 - accuracy: 0.5165 - f1_score: 0.2800 - val_loss: 0.9600 - val_accuracy: 0.5000 - val_f1_score: 0.2447 - lr: 0.0010\n",
      "Epoch 5/30\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.9268 - accuracy: 0.5596 - f1_score: 0.3928\n",
      "Epoch 5: val_f1_score improved from 0.24469 to 0.29338, saving model to lstm_avg_single.h5\n",
      "69/69 [==============================] - 11s 166ms/step - loss: 0.9268 - accuracy: 0.5596 - f1_score: 0.3928 - val_loss: 0.9643 - val_accuracy: 0.5261 - val_f1_score: 0.2934 - lr: 0.0010\n",
      "Epoch 6/30\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.8847 - accuracy: 0.5949 - f1_score: 0.4759\n",
      "Epoch 6: val_f1_score improved from 0.29338 to 0.32393, saving model to lstm_avg_single.h5\n",
      "69/69 [==============================] - 12s 173ms/step - loss: 0.8847 - accuracy: 0.5949 - f1_score: 0.4759 - val_loss: 0.9329 - val_accuracy: 0.5261 - val_f1_score: 0.3239 - lr: 0.0010\n",
      "Epoch 7/30\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.8226 - accuracy: 0.6475 - f1_score: 0.5559\n",
      "Epoch 7: val_f1_score improved from 0.32393 to 0.39098, saving model to lstm_avg_single.h5\n",
      "69/69 [==============================] - 12s 180ms/step - loss: 0.8226 - accuracy: 0.6475 - f1_score: 0.5559 - val_loss: 0.9412 - val_accuracy: 0.5224 - val_f1_score: 0.3910 - lr: 0.0010\n",
      "Epoch 8/30\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.7700 - accuracy: 0.6765 - f1_score: 0.6226\n",
      "Epoch 8: val_f1_score did not improve from 0.39098\n",
      "69/69 [==============================] - 12s 181ms/step - loss: 0.7700 - accuracy: 0.6765 - f1_score: 0.6226 - val_loss: 0.9648 - val_accuracy: 0.5560 - val_f1_score: 0.3782 - lr: 0.0010\n",
      "Epoch 9/30\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.7079 - accuracy: 0.7209 - f1_score: 0.6780\n",
      "Epoch 9: val_f1_score improved from 0.39098 to 0.49263, saving model to lstm_avg_single.h5\n",
      "69/69 [==============================] - 13s 187ms/step - loss: 0.7079 - accuracy: 0.7209 - f1_score: 0.6780 - val_loss: 0.9900 - val_accuracy: 0.5485 - val_f1_score: 0.4926 - lr: 0.0010\n",
      "Epoch 10/30\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.6583 - accuracy: 0.7367 - f1_score: 0.7067\n",
      "Epoch 10: val_f1_score did not improve from 0.49263\n",
      "69/69 [==============================] - 14s 200ms/step - loss: 0.6583 - accuracy: 0.7367 - f1_score: 0.7067 - val_loss: 0.9832 - val_accuracy: 0.5634 - val_f1_score: 0.4923 - lr: 0.0010\n",
      "Epoch 11/30\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.6115 - accuracy: 0.7599 - f1_score: 0.7416\n",
      "Epoch 11: val_f1_score did not improve from 0.49263\n",
      "69/69 [==============================] - 13s 186ms/step - loss: 0.6115 - accuracy: 0.7599 - f1_score: 0.7416 - val_loss: 1.0040 - val_accuracy: 0.5784 - val_f1_score: 0.4741 - lr: 0.0010\n",
      "Epoch 12/30\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.5529 - accuracy: 0.7821 - f1_score: 0.7706\n",
      "Epoch 12: val_f1_score improved from 0.49263 to 0.54461, saving model to lstm_avg_single.h5\n",
      "69/69 [==============================] - 13s 190ms/step - loss: 0.5529 - accuracy: 0.7821 - f1_score: 0.7706 - val_loss: 1.0608 - val_accuracy: 0.5709 - val_f1_score: 0.5446 - lr: 0.0010\n",
      "Epoch 13/30\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.4973 - accuracy: 0.8097 - f1_score: 0.7996\n",
      "Epoch 13: val_f1_score did not improve from 0.54461\n",
      "69/69 [==============================] - 12s 178ms/step - loss: 0.4973 - accuracy: 0.8097 - f1_score: 0.7996 - val_loss: 1.1260 - val_accuracy: 0.5075 - val_f1_score: 0.4209 - lr: 0.0010\n",
      "Epoch 14/30\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.4696 - accuracy: 0.8274 - f1_score: 0.8126\n",
      "Epoch 14: val_f1_score did not improve from 0.54461\n",
      "69/69 [==============================] - 13s 184ms/step - loss: 0.4696 - accuracy: 0.8274 - f1_score: 0.8126 - val_loss: 1.2724 - val_accuracy: 0.5336 - val_f1_score: 0.4971 - lr: 0.0010\n",
      "Epoch 15/30\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.4408 - accuracy: 0.8355 - f1_score: 0.8288\n",
      "Epoch 15: val_f1_score did not improve from 0.54461\n",
      "69/69 [==============================] - 13s 195ms/step - loss: 0.4408 - accuracy: 0.8355 - f1_score: 0.8288 - val_loss: 1.3563 - val_accuracy: 0.5149 - val_f1_score: 0.4895 - lr: 0.0010\n",
      "Epoch 16/30\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.3994 - accuracy: 0.8559 - f1_score: 0.8488\n",
      "Epoch 16: val_f1_score did not improve from 0.54461\n",
      "69/69 [==============================] - 13s 193ms/step - loss: 0.3994 - accuracy: 0.8559 - f1_score: 0.8488 - val_loss: 1.3191 - val_accuracy: 0.5560 - val_f1_score: 0.5142 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, GlobalAveragePooling1D\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_words, embedding_dim, weights=[embedding_matrix], input_length=100, trainable=False))\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(64, return_sequences=True))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model.compile(loss=CategoricalCrossentropy(), optimizer=RMSprop(learning_rate=0.001), metrics=['accuracy', f1_score])\n",
    "model.summary()\n",
    "\n",
    "# save best model when training\n",
    "checkpoint = ModelCheckpoint('lstm_avg_single.h5', monitor='val_f1_score', verbose=1, save_best_only=True, mode='max')\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "history = model.fit(X_train_padded, Y_train_encoded, epochs=30, batch_size=32, validation_data=(X_val_padded, Y_val_encoded), callbacks=[checkpoint, reduce_lr, early_stop])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 1s 173ms/step - loss: 0.9419 - accuracy: 0.6290 - f1_score: 0.6036\n",
      "Test loss:  0.9418776035308838\n",
      "Test accuracy:  0.6290322542190552\n",
      "Test f1 score:  0.6036418676376343\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "test_loss, test_acc, test_f1 = model.evaluate(X_test_padded, Y_test_encoded, batch_size=64)\n",
    "print('Test loss: ', test_loss)\n",
    "print('Test accuracy: ', test_acc)\n",
    "print('Test f1 score: ', test_f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 2s 148ms/step - loss: 0.9005 - accuracy: 0.5806 - f1_score: 0.5439\n",
      "Test loss:  0.9004533290863037\n",
      "Test accuracy:  0.5806451439857483\n",
      "Test f1 score:  0.5438573956489563\n"
     ]
    }
   ],
   "source": [
    "#evaluate the best model\n",
    "from tensorflow.keras.models import load_model\n",
    "best_model = load_model('lstm_avg_single.h5', custom_objects={'f1_score': f1_score})\n",
    "test_loss, test_acc, test_f1 = best_model.evaluate(X_test_padded, Y_test_encoded, batch_size=64)\n",
    "print('Test loss: ', test_loss)\n",
    "print('Test accuracy: ', test_acc)\n",
    "print('Test f1 score: ', test_f1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from output_multi.csv\n",
    "import pandas as pd\n",
    "df = pd.read_csv('./output_multi.csv')\n",
    "\n",
    "X = df['RawText'].tolist()\n",
    "Y = df['Label'].tolist()\n",
    "\n",
    "# split data, 8:1:1\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.2, random_state=42)\n",
    "X_test, X_val, Y_test, Y_val = train_test_split(X_test, Y_test, test_size=.5, random_state=42)\n",
    "\n",
    "X_train = [str(text) for text in X_train]\n",
    "X_test = [str(text) for text in X_test]\n",
    "X_val = [str(text) for text in X_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# Load the GLOVE embeddings\n",
    "embedding_path = '../mgnns/glove.6B.300d.txt'\n",
    "embedding_index = {}\n",
    "with open(embedding_path, encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embedding_index[word] = coefs\n",
    "\n",
    "# Define the tokenizer and fit on the text data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Convert the text to sequences of integers and pad to a length of 100\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_train_padded = pad_sequences(X_train_sequences, maxlen=100)\n",
    "\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "X_test_padded = pad_sequences(X_test_sequences, maxlen=100)\n",
    "\n",
    "X_val_sequences = tokenizer.texts_to_sequences(X_val)\n",
    "X_val_padded = pad_sequences(X_val_sequences, maxlen=100)\n",
    "\n",
    "# Create an embedding matrix for the words in the tokenizer\n",
    "word_index = tokenizer.word_index\n",
    "embedding_dim = 300\n",
    "num_words = len(word_index) + 1\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        embedding_matrix[i] = np.random.normal(size=(embedding_dim,))\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "lb = LabelBinarizer()\n",
    "Y_train_encoded = lb.fit_transform(Y_train)\n",
    "Y_val_encoded = lb.transform(Y_val)\n",
    "Y_test_encoded = lb.transform(Y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def f1_score(y_true, y_pred): \n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 100, 300)          4329000   \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 100, 128)          219648    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 100, 128)          0         \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 100, 64)           49408     \n",
      "                                                                 \n",
      " global_average_pooling1d_1   (None, 64)               0         \n",
      " (GlobalAveragePooling1D)                                        \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,602,411\n",
      "Trainable params: 273,411\n",
      "Non-trainable params: 4,329,000\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.7356 - accuracy: 0.7043 - f1_score: 0.6892\n",
      "Epoch 1: val_f1_score improved from -inf to 0.70295, saving model to lstm_avg_multi.h5\n",
      "429/429 [==============================] - 53s 116ms/step - loss: 0.7356 - accuracy: 0.7043 - f1_score: 0.6892 - val_loss: 0.7020 - val_accuracy: 0.7095 - val_f1_score: 0.7030 - lr: 0.0010\n",
      "Epoch 2/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.6967 - accuracy: 0.7073 - f1_score: 0.7042\n",
      "Epoch 2: val_f1_score did not improve from 0.70295\n",
      "429/429 [==============================] - 51s 120ms/step - loss: 0.6967 - accuracy: 0.7073 - f1_score: 0.7042 - val_loss: 0.6847 - val_accuracy: 0.7095 - val_f1_score: 0.7004 - lr: 0.0010\n",
      "Epoch 3/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.6798 - accuracy: 0.7071 - f1_score: 0.7032\n",
      "Epoch 3: val_f1_score improved from 0.70295 to 0.70932, saving model to lstm_avg_multi.h5\n",
      "429/429 [==============================] - 51s 120ms/step - loss: 0.6798 - accuracy: 0.7071 - f1_score: 0.7032 - val_loss: 0.6862 - val_accuracy: 0.7095 - val_f1_score: 0.7093 - lr: 0.0010\n",
      "Epoch 4/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.6649 - accuracy: 0.7076 - f1_score: 0.7027\n",
      "Epoch 4: val_f1_score did not improve from 0.70932\n",
      "429/429 [==============================] - 44s 102ms/step - loss: 0.6649 - accuracy: 0.7076 - f1_score: 0.7027 - val_loss: 0.6722 - val_accuracy: 0.7095 - val_f1_score: 0.7079 - lr: 0.0010\n",
      "Epoch 5/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.6526 - accuracy: 0.7078 - f1_score: 0.7041\n",
      "Epoch 5: val_f1_score did not improve from 0.70932\n",
      "429/429 [==============================] - 42s 98ms/step - loss: 0.6526 - accuracy: 0.7078 - f1_score: 0.7041 - val_loss: 0.6648 - val_accuracy: 0.7124 - val_f1_score: 0.7046 - lr: 0.0010\n",
      "Epoch 6/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.6415 - accuracy: 0.7101 - f1_score: 0.7037\n",
      "Epoch 6: val_f1_score improved from 0.70932 to 0.71067, saving model to lstm_avg_multi.h5\n",
      "429/429 [==============================] - 44s 103ms/step - loss: 0.6415 - accuracy: 0.7101 - f1_score: 0.7037 - val_loss: 0.6679 - val_accuracy: 0.7124 - val_f1_score: 0.7107 - lr: 0.0010\n",
      "Epoch 7/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.6313 - accuracy: 0.7091 - f1_score: 0.7051\n",
      "Epoch 7: val_f1_score did not improve from 0.71067\n",
      "429/429 [==============================] - 49s 115ms/step - loss: 0.6313 - accuracy: 0.7091 - f1_score: 0.7051 - val_loss: 0.6826 - val_accuracy: 0.7135 - val_f1_score: 0.7100 - lr: 0.0010\n",
      "Epoch 8/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.6192 - accuracy: 0.7181 - f1_score: 0.7108\n",
      "Epoch 8: val_f1_score did not improve from 0.71067\n",
      "429/429 [==============================] - 54s 125ms/step - loss: 0.6192 - accuracy: 0.7181 - f1_score: 0.7108 - val_loss: 0.6678 - val_accuracy: 0.7159 - val_f1_score: 0.7034 - lr: 0.0010\n",
      "Epoch 9/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.6056 - accuracy: 0.7202 - f1_score: 0.7126\n",
      "Epoch 9: val_f1_score did not improve from 0.71067\n",
      "429/429 [==============================] - 52s 120ms/step - loss: 0.6056 - accuracy: 0.7202 - f1_score: 0.7126 - val_loss: 0.6761 - val_accuracy: 0.7048 - val_f1_score: 0.6893 - lr: 0.0010\n",
      "Epoch 10/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.5880 - accuracy: 0.7326 - f1_score: 0.7262\n",
      "Epoch 10: val_f1_score did not improve from 0.71067\n",
      "429/429 [==============================] - 49s 114ms/step - loss: 0.5880 - accuracy: 0.7326 - f1_score: 0.7262 - val_loss: 0.6958 - val_accuracy: 0.6849 - val_f1_score: 0.6765 - lr: 0.0010\n",
      "Epoch 11/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.5731 - accuracy: 0.7415 - f1_score: 0.7353\n",
      "Epoch 11: val_f1_score did not improve from 0.71067\n",
      "429/429 [==============================] - 44s 103ms/step - loss: 0.5731 - accuracy: 0.7415 - f1_score: 0.7353 - val_loss: 0.7236 - val_accuracy: 0.7001 - val_f1_score: 0.6957 - lr: 0.0010\n",
      "Epoch 12/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.5494 - accuracy: 0.7499 - f1_score: 0.7438\n",
      "Epoch 12: val_f1_score did not improve from 0.71067\n",
      "429/429 [==============================] - 52s 121ms/step - loss: 0.5494 - accuracy: 0.7499 - f1_score: 0.7438 - val_loss: 0.7143 - val_accuracy: 0.6937 - val_f1_score: 0.6885 - lr: 0.0010\n",
      "Epoch 13/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.5237 - accuracy: 0.7679 - f1_score: 0.7633\n",
      "Epoch 13: val_f1_score did not improve from 0.71067\n",
      "429/429 [==============================] - 47s 109ms/step - loss: 0.5237 - accuracy: 0.7679 - f1_score: 0.7633 - val_loss: 0.7713 - val_accuracy: 0.6989 - val_f1_score: 0.6946 - lr: 0.0010\n",
      "Epoch 14/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.4985 - accuracy: 0.7806 - f1_score: 0.7767\n",
      "Epoch 14: val_f1_score did not improve from 0.71067\n",
      "429/429 [==============================] - 45s 105ms/step - loss: 0.4985 - accuracy: 0.7806 - f1_score: 0.7767 - val_loss: 0.7808 - val_accuracy: 0.6838 - val_f1_score: 0.6803 - lr: 0.0010\n",
      "Epoch 15/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.4722 - accuracy: 0.7941 - f1_score: 0.7901\n",
      "Epoch 15: val_f1_score did not improve from 0.71067\n",
      "429/429 [==============================] - 45s 106ms/step - loss: 0.4722 - accuracy: 0.7941 - f1_score: 0.7901 - val_loss: 0.7957 - val_accuracy: 0.6604 - val_f1_score: 0.6541 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, GlobalAveragePooling1D\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_words, embedding_dim, weights=[embedding_matrix], input_length=100, trainable=False))\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(64, return_sequences=True))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model.compile(loss=CategoricalCrossentropy(), optimizer=RMSprop(learning_rate=0.001), metrics=['accuracy', f1_score])\n",
    "model.summary()\n",
    "\n",
    "# save best model when training\n",
    "checkpoint = ModelCheckpoint('lstm_avg_multi.h5', monitor='val_f1_score', verbose=1, save_best_only=True, mode='max')\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "history = model.fit(X_train_padded, Y_train_encoded, epochs=30, batch_size=32, validation_data=(X_val_padded, Y_val_encoded), callbacks=[checkpoint, reduce_lr, early_stop])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 2s 83ms/step - loss: 0.7731 - accuracy: 0.6597 - f1_score: 0.6521\n",
      "Test loss:  0.7731484770774841\n",
      "Test accuracy:  0.6596614122390747\n",
      "Test f1 score:  0.6521373391151428\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "test_loss, test_acc, test_f1 = model.evaluate(X_test_padded, Y_test_encoded, batch_size=64)\n",
    "print('Test loss: ', test_loss)\n",
    "print('Test accuracy: ', test_acc)\n",
    "print('Test f1 score: ', test_f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 3s 72ms/step - loss: 0.6648 - accuracy: 0.7075 - f1_score: 0.7078\n",
      "Test loss:  0.6647664904594421\n",
      "Test accuracy:  0.707530677318573\n",
      "Test f1 score:  0.7078070640563965\n"
     ]
    }
   ],
   "source": [
    "#evaluate the best model\n",
    "from tensorflow.keras.models import load_model\n",
    "best_model = load_model('lstm_avg_multi.h5', custom_objects={'f1_score': f1_score})\n",
    "test_loss, test_acc, test_f1 = best_model.evaluate(X_test_padded, Y_test_encoded, batch_size=64)\n",
    "print('Test loss: ', test_loss)\n",
    "print('Test accuracy: ', test_acc)\n",
    "print('Test f1 score: ', test_f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MGNNS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
