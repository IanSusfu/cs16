{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_data(file_path):\n",
    "    X_data = []\n",
    "    Y_data = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            example = json.loads(line.strip())\n",
    "            X_data.append(example['text'])\n",
    "            Y_data.append(example['label'])\n",
    "    return X_data, Y_data\n",
    "\n",
    "X_train, Y_train = load_data('../mgnns/train_all_anno.json')\n",
    "X_test, Y_test = load_data('../mgnns/test_all_anno.json')\n",
    "X_val, Y_val = load_data('../mgnns/val_all_anno.json')\n",
    "\n",
    "X_train = [x.lower() for x in X_train]\n",
    "X_test = [x.lower() for x in X_test]\n",
    "X_val = [x.lower() for x in X_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# Load the GLOVE embeddings\n",
    "embedding_path = '../mgnns/glove.6B.300d.txt'\n",
    "embedding_index = {}\n",
    "with open(embedding_path, encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embedding_index[word] = coefs\n",
    "\n",
    "# Define the tokenizer and fit on the text data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Convert the text to sequences of integers and pad to a length of 100\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_train_padded = pad_sequences(X_train_sequences, maxlen=100)\n",
    "\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "X_test_padded = pad_sequences(X_test_sequences, maxlen=100)\n",
    "\n",
    "X_val_sequences = tokenizer.texts_to_sequences(X_val)\n",
    "X_val_padded = pad_sequences(X_val_sequences, maxlen=100)\n",
    "\n",
    "# Create an embedding matrix for the words in the tokenizer\n",
    "word_index = tokenizer.word_index\n",
    "embedding_dim = 300\n",
    "num_words = len(word_index) + 1\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        embedding_matrix[i] = np.random.normal(size=(embedding_dim,))\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "lb = LabelBinarizer()\n",
    "Y_train_encoded = lb.fit_transform(Y_train)\n",
    "Y_val_encoded = lb.transform(Y_val)\n",
    "Y_test_encoded = lb.transform(Y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def f1_score(y_true, y_pred): \n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 100, 300)          2571000   \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 100, 256)         439296    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 100, 256)          0         \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 128)              164352    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,183,099\n",
      "Trainable params: 612,099\n",
      "Non-trainable params: 2,571,000\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.9922 - accuracy: 0.5011 - f1_score: 0.2958\n",
      "Epoch 1: val_f1_score improved from -inf to 0.41897, saving model to bilstm.h5\n",
      "69/69 [==============================] - 16s 165ms/step - loss: 0.9922 - accuracy: 0.5011 - f1_score: 0.2958 - val_loss: 0.9121 - val_accuracy: 0.5448 - val_f1_score: 0.4190 - lr: 0.0010\n",
      "Epoch 2/30\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.8825 - accuracy: 0.6013 - f1_score: 0.5329\n",
      "Epoch 2: val_f1_score improved from 0.41897 to 0.47325, saving model to bilstm.h5\n",
      "69/69 [==============================] - 10s 144ms/step - loss: 0.8825 - accuracy: 0.6013 - f1_score: 0.5329 - val_loss: 0.8802 - val_accuracy: 0.6231 - val_f1_score: 0.4733 - lr: 0.0010\n",
      "Epoch 3/30\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.7883 - accuracy: 0.6724 - f1_score: 0.6195\n",
      "Epoch 3: val_f1_score improved from 0.47325 to 0.56012, saving model to bilstm.h5\n",
      "69/69 [==============================] - 10s 145ms/step - loss: 0.7883 - accuracy: 0.6724 - f1_score: 0.6195 - val_loss: 0.8881 - val_accuracy: 0.6082 - val_f1_score: 0.5601 - lr: 0.0010\n",
      "Epoch 4/30\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.6800 - accuracy: 0.7386 - f1_score: 0.7079\n",
      "Epoch 4: val_f1_score improved from 0.56012 to 0.56501, saving model to bilstm.h5\n",
      "69/69 [==============================] - 11s 163ms/step - loss: 0.6800 - accuracy: 0.7386 - f1_score: 0.7079 - val_loss: 0.9679 - val_accuracy: 0.5784 - val_f1_score: 0.5650 - lr: 0.0010\n",
      "Epoch 5/30\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.5673 - accuracy: 0.7789 - f1_score: 0.7661\n",
      "Epoch 5: val_f1_score did not improve from 0.56501\n",
      "69/69 [==============================] - 11s 159ms/step - loss: 0.5673 - accuracy: 0.7789 - f1_score: 0.7661 - val_loss: 1.1494 - val_accuracy: 0.5560 - val_f1_score: 0.5052 - lr: 0.0010\n",
      "Epoch 6/30\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.4848 - accuracy: 0.8174 - f1_score: 0.8135\n",
      "Epoch 6: val_f1_score did not improve from 0.56501\n",
      "69/69 [==============================] - 11s 160ms/step - loss: 0.4848 - accuracy: 0.8174 - f1_score: 0.8135 - val_loss: 1.0740 - val_accuracy: 0.5672 - val_f1_score: 0.5460 - lr: 0.0010\n",
      "Epoch 7/30\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.3830 - accuracy: 0.8614 - f1_score: 0.8546\n",
      "Epoch 7: val_f1_score did not improve from 0.56501\n",
      "69/69 [==============================] - 11s 161ms/step - loss: 0.3830 - accuracy: 0.8614 - f1_score: 0.8546 - val_loss: 1.3658 - val_accuracy: 0.5187 - val_f1_score: 0.4912 - lr: 0.0010\n",
      "Epoch 8/30\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.3072 - accuracy: 0.8854 - f1_score: 0.8866\n",
      "Epoch 8: val_f1_score did not improve from 0.56501\n",
      "69/69 [==============================] - 11s 159ms/step - loss: 0.3072 - accuracy: 0.8854 - f1_score: 0.8866 - val_loss: 1.5857 - val_accuracy: 0.5858 - val_f1_score: 0.5580 - lr: 0.0010\n",
      "Epoch 9/30\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.2552 - accuracy: 0.9089 - f1_score: 0.9096\n",
      "Epoch 9: val_f1_score improved from 0.56501 to 0.58938, saving model to bilstm.h5\n",
      "69/69 [==============================] - 11s 161ms/step - loss: 0.2552 - accuracy: 0.9089 - f1_score: 0.9096 - val_loss: 1.4330 - val_accuracy: 0.5933 - val_f1_score: 0.5894 - lr: 0.0010\n",
      "Epoch 10/30\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.2169 - accuracy: 0.9221 - f1_score: 0.9238\n",
      "Epoch 10: val_f1_score did not improve from 0.58938\n",
      "69/69 [==============================] - 11s 163ms/step - loss: 0.2169 - accuracy: 0.9221 - f1_score: 0.9238 - val_loss: 1.7971 - val_accuracy: 0.5597 - val_f1_score: 0.5475 - lr: 0.0010\n",
      "Epoch 11/30\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.1857 - accuracy: 0.9316 - f1_score: 0.9324\n",
      "Epoch 11: val_f1_score did not improve from 0.58938\n",
      "69/69 [==============================] - 11s 159ms/step - loss: 0.1857 - accuracy: 0.9316 - f1_score: 0.9324 - val_loss: 1.5241 - val_accuracy: 0.5410 - val_f1_score: 0.5203 - lr: 0.0010\n",
      "Epoch 12/30\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.1512 - accuracy: 0.9533 - f1_score: 0.9513\n",
      "Epoch 12: val_f1_score did not improve from 0.58938\n",
      "69/69 [==============================] - 11s 160ms/step - loss: 0.1512 - accuracy: 0.9533 - f1_score: 0.9513 - val_loss: 1.7216 - val_accuracy: 0.5858 - val_f1_score: 0.5731 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_words, embedding_dim, weights=[embedding_matrix], input_length=100, trainable=False))\n",
    "model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model.compile(loss=CategoricalCrossentropy(), optimizer=RMSprop(learning_rate=0.001), metrics=['accuracy', f1_score])\n",
    "model.summary()\n",
    "\n",
    "# save best model when training\n",
    "checkpoint = ModelCheckpoint('bilstm.h5', monitor='val_f1_score', verbose=1, save_best_only=True, mode='max')\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "history = model.fit(X_train_padded, Y_train_encoded, epochs=30, batch_size=32, validation_data=(X_val_padded, Y_val_encoded), callbacks=[checkpoint, reduce_lr, early_stop])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 1s 121ms/step - loss: 1.6201 - accuracy: 0.5991 - f1_score: 0.5967\n",
      "Test loss:  1.6201343536376953\n",
      "Test accuracy:  0.599078357219696\n",
      "Test f1 score:  0.5966647267341614\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "test_loss, test_acc, test_f1 = model.evaluate(X_test_padded, Y_test_encoded, batch_size=64)\n",
    "print('Test loss: ', test_loss)\n",
    "print('Test accuracy: ', test_acc)\n",
    "print('Test f1 score: ', test_f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 2s 93ms/step - loss: 1.1842 - accuracy: 0.6290 - f1_score: 0.6232\n",
      "Test loss:  1.1842362880706787\n",
      "Test accuracy:  0.6290322542190552\n",
      "Test f1 score:  0.6232410073280334\n"
     ]
    }
   ],
   "source": [
    "#evaluate the best model\n",
    "from tensorflow.keras.models import load_model\n",
    "best_model = load_model('bilstm.h5', custom_objects={'f1_score': f1_score})\n",
    "test_loss, test_acc, test_f1 = best_model.evaluate(X_test_padded, Y_test_encoded, batch_size=64)\n",
    "print('Test loss: ', test_loss)\n",
    "print('Test accuracy: ', test_acc)\n",
    "print('Test f1 score: ', test_f1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from output_multi.csv\n",
    "import pandas as pd\n",
    "df = pd.read_csv('./output_multi.csv')\n",
    "\n",
    "X = df['RawText'].tolist()\n",
    "Y = df['Label'].tolist()\n",
    "\n",
    "# split data, 8:1:1\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.2, random_state=42)\n",
    "X_test, X_val, Y_test, Y_val = train_test_split(X_test, Y_test, test_size=.5, random_state=42)\n",
    "\n",
    "X_train = [str(text) for text in X_train]\n",
    "X_test = [str(text) for text in X_test]\n",
    "X_val = [str(text) for text in X_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# Load the GLOVE embeddings\n",
    "embedding_path = '../mgnns/glove.6B.300d.txt'\n",
    "embedding_index = {}\n",
    "with open(embedding_path, encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embedding_index[word] = coefs\n",
    "\n",
    "# Define the tokenizer and fit on the text data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Convert the text to sequences of integers and pad to a length of 100\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_train_padded = pad_sequences(X_train_sequences, maxlen=100)\n",
    "\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "X_test_padded = pad_sequences(X_test_sequences, maxlen=100)\n",
    "\n",
    "X_val_sequences = tokenizer.texts_to_sequences(X_val)\n",
    "X_val_padded = pad_sequences(X_val_sequences, maxlen=100)\n",
    "\n",
    "# Create an embedding matrix for the words in the tokenizer\n",
    "word_index = tokenizer.word_index\n",
    "embedding_dim = 300\n",
    "num_words = len(word_index) + 1\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        embedding_matrix[i] = np.random.normal(size=(embedding_dim,))\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "lb = LabelBinarizer()\n",
    "Y_train_encoded = lb.fit_transform(Y_train)\n",
    "Y_val_encoded = lb.transform(Y_val)\n",
    "Y_test_encoded = lb.transform(Y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def f1_score(y_true, y_pred): \n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 100, 300)          4329000   \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 100, 256)         439296    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 100, 256)          0         \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 128)              164352    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,941,099\n",
      "Trainable params: 612,099\n",
      "Non-trainable params: 4,329,000\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.7138 - accuracy: 0.7021 - f1_score: 0.6827\n",
      "Epoch 1: val_f1_score improved from -inf to 0.54662, saving model to bilstm_multi.h5\n",
      "429/429 [==============================] - 68s 148ms/step - loss: 0.7138 - accuracy: 0.7021 - f1_score: 0.6827 - val_loss: 0.7796 - val_accuracy: 0.7083 - val_f1_score: 0.5466 - lr: 0.0010\n",
      "Epoch 2/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.6670 - accuracy: 0.7112 - f1_score: 0.7011\n",
      "Epoch 2: val_f1_score improved from 0.54662 to 0.71415, saving model to bilstm_multi.h5\n",
      "429/429 [==============================] - 65s 152ms/step - loss: 0.6670 - accuracy: 0.7112 - f1_score: 0.7011 - val_loss: 0.6758 - val_accuracy: 0.7170 - val_f1_score: 0.7141 - lr: 0.0010\n",
      "Epoch 3/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.6495 - accuracy: 0.7146 - f1_score: 0.7059\n",
      "Epoch 3: val_f1_score did not improve from 0.71415\n",
      "429/429 [==============================] - 66s 154ms/step - loss: 0.6495 - accuracy: 0.7146 - f1_score: 0.7059 - val_loss: 0.6923 - val_accuracy: 0.7095 - val_f1_score: 0.6855 - lr: 0.0010\n",
      "Epoch 4/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.6286 - accuracy: 0.7179 - f1_score: 0.7085\n",
      "Epoch 4: val_f1_score did not improve from 0.71415\n",
      "429/429 [==============================] - 66s 153ms/step - loss: 0.6286 - accuracy: 0.7179 - f1_score: 0.7085 - val_loss: 0.6738 - val_accuracy: 0.7106 - val_f1_score: 0.7082 - lr: 0.0010\n",
      "Epoch 5/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.6072 - accuracy: 0.7269 - f1_score: 0.7222\n",
      "Epoch 5: val_f1_score did not improve from 0.71415\n",
      "429/429 [==============================] - 67s 155ms/step - loss: 0.6072 - accuracy: 0.7269 - f1_score: 0.7222 - val_loss: 0.7312 - val_accuracy: 0.6709 - val_f1_score: 0.6604 - lr: 0.0010\n",
      "Epoch 6/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.5910 - accuracy: 0.7320 - f1_score: 0.7274\n",
      "Epoch 6: val_f1_score did not improve from 0.71415\n",
      "429/429 [==============================] - 67s 157ms/step - loss: 0.5910 - accuracy: 0.7320 - f1_score: 0.7274 - val_loss: 0.7252 - val_accuracy: 0.7130 - val_f1_score: 0.7103 - lr: 0.0010\n",
      "Epoch 7/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.5706 - accuracy: 0.7413 - f1_score: 0.7367\n",
      "Epoch 7: val_f1_score did not improve from 0.71415\n",
      "429/429 [==============================] - 67s 157ms/step - loss: 0.5706 - accuracy: 0.7413 - f1_score: 0.7367 - val_loss: 0.7801 - val_accuracy: 0.7165 - val_f1_score: 0.7125 - lr: 0.0010\n",
      "Epoch 8/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.5393 - accuracy: 0.7588 - f1_score: 0.7555\n",
      "Epoch 8: val_f1_score did not improve from 0.71415\n",
      "429/429 [==============================] - 68s 158ms/step - loss: 0.5393 - accuracy: 0.7588 - f1_score: 0.7555 - val_loss: 0.7370 - val_accuracy: 0.7135 - val_f1_score: 0.7086 - lr: 0.0010\n",
      "Epoch 9/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.5102 - accuracy: 0.7733 - f1_score: 0.7706\n",
      "Epoch 9: val_f1_score did not improve from 0.71415\n",
      "429/429 [==============================] - 70s 163ms/step - loss: 0.5102 - accuracy: 0.7733 - f1_score: 0.7706 - val_loss: 0.7815 - val_accuracy: 0.7130 - val_f1_score: 0.7103 - lr: 0.0010\n",
      "Epoch 10/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.4743 - accuracy: 0.7929 - f1_score: 0.7911\n",
      "Epoch 10: val_f1_score did not improve from 0.71415\n",
      "429/429 [==============================] - 67s 157ms/step - loss: 0.4743 - accuracy: 0.7929 - f1_score: 0.7911 - val_loss: 0.8525 - val_accuracy: 0.6750 - val_f1_score: 0.6650 - lr: 0.0010\n",
      "Epoch 11/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.4427 - accuracy: 0.8056 - f1_score: 0.8051\n",
      "Epoch 11: val_f1_score did not improve from 0.71415\n",
      "429/429 [==============================] - 69s 160ms/step - loss: 0.4427 - accuracy: 0.8056 - f1_score: 0.8051 - val_loss: 1.0214 - val_accuracy: 0.6645 - val_f1_score: 0.6509 - lr: 0.0010\n",
      "Epoch 12/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.4049 - accuracy: 0.8249 - f1_score: 0.8231\n",
      "Epoch 12: val_f1_score did not improve from 0.71415\n",
      "429/429 [==============================] - 69s 160ms/step - loss: 0.4049 - accuracy: 0.8249 - f1_score: 0.8231 - val_loss: 0.9758 - val_accuracy: 0.6803 - val_f1_score: 0.6757 - lr: 0.0010\n",
      "Epoch 13/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.3694 - accuracy: 0.8431 - f1_score: 0.8410\n",
      "Epoch 13: val_f1_score did not improve from 0.71415\n",
      "429/429 [==============================] - 66s 155ms/step - loss: 0.3694 - accuracy: 0.8431 - f1_score: 0.8410 - val_loss: 1.1617 - val_accuracy: 0.6908 - val_f1_score: 0.6853 - lr: 0.0010\n",
      "Epoch 14/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.3398 - accuracy: 0.8572 - f1_score: 0.8563\n",
      "Epoch 14: val_f1_score did not improve from 0.71415\n",
      "429/429 [==============================] - 67s 155ms/step - loss: 0.3398 - accuracy: 0.8572 - f1_score: 0.8563 - val_loss: 1.0644 - val_accuracy: 0.6861 - val_f1_score: 0.6804 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_words, embedding_dim, weights=[embedding_matrix], input_length=100, trainable=False))\n",
    "model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model.compile(loss=CategoricalCrossentropy(), optimizer=RMSprop(learning_rate=0.001), metrics=['accuracy', f1_score])\n",
    "model.summary()\n",
    "\n",
    "# save best model when training\n",
    "checkpoint = ModelCheckpoint('bilstm_multi.h5', monitor='val_f1_score', verbose=1, save_best_only=True, mode='max')\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "history = model.fit(X_train_padded, Y_train_encoded, epochs=30, batch_size=32, validation_data=(X_val_padded, Y_val_encoded), callbacks=[checkpoint, reduce_lr, early_stop])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 3s 98ms/step - loss: 1.0850 - accuracy: 0.6871 - f1_score: 0.6799\n",
      "Test loss:  1.0849939584732056\n",
      "Test accuracy:  0.6870986819267273\n",
      "Test f1 score:  0.6798893809318542\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "test_loss, test_acc, test_f1 = model.evaluate(X_test_padded, Y_test_encoded, batch_size=64)\n",
    "print('Test loss: ', test_loss)\n",
    "print('Test accuracy: ', test_acc)\n",
    "print('Test f1 score: ', test_f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 3s 81ms/step - loss: 0.6698 - accuracy: 0.7093 - f1_score: 0.7079\n",
      "Test loss:  0.6697537899017334\n",
      "Test accuracy:  0.7092819809913635\n",
      "Test f1 score:  0.7079153656959534\n"
     ]
    }
   ],
   "source": [
    "#evaluate the best model\n",
    "from tensorflow.keras.models import load_model\n",
    "best_model = load_model('bilstm_multi.h5', custom_objects={'f1_score': f1_score})\n",
    "test_loss, test_acc, test_f1 = best_model.evaluate(X_test_padded, Y_test_encoded, batch_size=64)\n",
    "print('Test loss: ', test_loss)\n",
    "print('Test accuracy: ', test_acc)\n",
    "print('Test f1 score: ', test_f1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.7135 - accuracy: 0.7037 - f1_score: 0.6878\n",
      "Epoch 1: val_f1_score improved from -inf to 0.69344, saving model to bilstm_multi_0.h5\n",
      "429/429 [==============================] - 72s 158ms/step - loss: 0.7135 - accuracy: 0.7037 - f1_score: 0.6878 - val_loss: 0.6998 - val_accuracy: 0.7118 - val_f1_score: 0.6934 - lr: 0.0010\n",
      "Epoch 2/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.6652 - accuracy: 0.7113 - f1_score: 0.7022\n",
      "Epoch 2: val_f1_score did not improve from 0.69344\n",
      "429/429 [==============================] - 65s 152ms/step - loss: 0.6652 - accuracy: 0.7113 - f1_score: 0.7022 - val_loss: 0.7195 - val_accuracy: 0.6884 - val_f1_score: 0.6521 - lr: 0.0010\n",
      "Epoch 3/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.6497 - accuracy: 0.7140 - f1_score: 0.7061\n",
      "Epoch 3: val_f1_score improved from 0.69344 to 0.71341, saving model to bilstm_multi_0.h5\n",
      "429/429 [==============================] - 65s 151ms/step - loss: 0.6497 - accuracy: 0.7140 - f1_score: 0.7061 - val_loss: 0.6760 - val_accuracy: 0.7153 - val_f1_score: 0.7134 - lr: 0.0010\n",
      "Epoch 4/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.6306 - accuracy: 0.7193 - f1_score: 0.7147\n",
      "Epoch 4: val_f1_score improved from 0.71341 to 0.71430, saving model to bilstm_multi_0.h5\n",
      "429/429 [==============================] - 65s 152ms/step - loss: 0.6306 - accuracy: 0.7193 - f1_score: 0.7147 - val_loss: 0.6789 - val_accuracy: 0.7188 - val_f1_score: 0.7143 - lr: 0.0010\n",
      "Epoch 5/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.6080 - accuracy: 0.7266 - f1_score: 0.7199\n",
      "Epoch 5: val_f1_score did not improve from 0.71430\n",
      "429/429 [==============================] - 65s 152ms/step - loss: 0.6080 - accuracy: 0.7266 - f1_score: 0.7199 - val_loss: 0.7087 - val_accuracy: 0.7153 - val_f1_score: 0.7124 - lr: 0.0010\n",
      "Epoch 6/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.5880 - accuracy: 0.7385 - f1_score: 0.7327\n",
      "Epoch 6: val_f1_score did not improve from 0.71430\n",
      "429/429 [==============================] - 65s 152ms/step - loss: 0.5880 - accuracy: 0.7385 - f1_score: 0.7327 - val_loss: 0.7237 - val_accuracy: 0.6879 - val_f1_score: 0.6721 - lr: 0.0010\n",
      "Epoch 7/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.5630 - accuracy: 0.7480 - f1_score: 0.7442\n",
      "Epoch 7: val_f1_score did not improve from 0.71430\n",
      "429/429 [==============================] - 66s 153ms/step - loss: 0.5630 - accuracy: 0.7480 - f1_score: 0.7442 - val_loss: 0.7136 - val_accuracy: 0.7147 - val_f1_score: 0.7020 - lr: 0.0010\n",
      "Epoch 8/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.5288 - accuracy: 0.7674 - f1_score: 0.7632\n",
      "Epoch 8: val_f1_score did not improve from 0.71430\n",
      "429/429 [==============================] - 65s 152ms/step - loss: 0.5288 - accuracy: 0.7674 - f1_score: 0.7632 - val_loss: 0.7258 - val_accuracy: 0.7013 - val_f1_score: 0.7006 - lr: 0.0010\n",
      "Epoch 9/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.4955 - accuracy: 0.7823 - f1_score: 0.7806\n",
      "Epoch 9: val_f1_score did not improve from 0.71430\n",
      "429/429 [==============================] - 65s 152ms/step - loss: 0.4955 - accuracy: 0.7823 - f1_score: 0.7806 - val_loss: 0.7736 - val_accuracy: 0.7112 - val_f1_score: 0.7061 - lr: 0.0010\n",
      "Epoch 10/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.4718 - accuracy: 0.7955 - f1_score: 0.7938\n",
      "Epoch 10: val_f1_score did not improve from 0.71430\n",
      "429/429 [==============================] - 67s 156ms/step - loss: 0.4718 - accuracy: 0.7955 - f1_score: 0.7938 - val_loss: 0.8102 - val_accuracy: 0.6896 - val_f1_score: 0.6832 - lr: 0.0010\n",
      "Epoch 11/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.4327 - accuracy: 0.8118 - f1_score: 0.8111\n",
      "Epoch 11: val_f1_score did not improve from 0.71430\n",
      "429/429 [==============================] - 68s 159ms/step - loss: 0.4327 - accuracy: 0.8118 - f1_score: 0.8111 - val_loss: 0.8662 - val_accuracy: 0.6750 - val_f1_score: 0.6646 - lr: 0.0010\n",
      "Epoch 12/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.4106 - accuracy: 0.8241 - f1_score: 0.8219\n",
      "Epoch 12: val_f1_score did not improve from 0.71430\n",
      "429/429 [==============================] - 70s 164ms/step - loss: 0.4106 - accuracy: 0.8241 - f1_score: 0.8219 - val_loss: 1.0786 - val_accuracy: 0.6867 - val_f1_score: 0.6834 - lr: 0.0010\n",
      "Epoch 13/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.3676 - accuracy: 0.8394 - f1_score: 0.8384\n",
      "Epoch 13: val_f1_score did not improve from 0.71430\n",
      "429/429 [==============================] - 69s 161ms/step - loss: 0.3676 - accuracy: 0.8394 - f1_score: 0.8384 - val_loss: 1.0628 - val_accuracy: 0.6820 - val_f1_score: 0.6801 - lr: 0.0010\n",
      "Epoch 1/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.7166 - accuracy: 0.7007 - f1_score: 0.6833\n",
      "Epoch 1: val_f1_score improved from -inf to 0.70826, saving model to bilstm_multi_1.h5\n",
      "429/429 [==============================] - 92s 203ms/step - loss: 0.7166 - accuracy: 0.7007 - f1_score: 0.6833 - val_loss: 0.6704 - val_accuracy: 0.7095 - val_f1_score: 0.7083 - lr: 0.0010\n",
      "Epoch 2/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.6647 - accuracy: 0.7104 - f1_score: 0.7022\n",
      "Epoch 2: val_f1_score did not improve from 0.70826\n",
      "429/429 [==============================] - 87s 202ms/step - loss: 0.6647 - accuracy: 0.7104 - f1_score: 0.7022 - val_loss: 0.6746 - val_accuracy: 0.7089 - val_f1_score: 0.7060 - lr: 0.0010\n",
      "Epoch 3/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.6481 - accuracy: 0.7161 - f1_score: 0.7062\n",
      "Epoch 3: val_f1_score did not improve from 0.70826\n",
      "429/429 [==============================] - 86s 200ms/step - loss: 0.6481 - accuracy: 0.7161 - f1_score: 0.7062 - val_loss: 0.6922 - val_accuracy: 0.6954 - val_f1_score: 0.6830 - lr: 0.0010\n",
      "Epoch 4/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.6280 - accuracy: 0.7174 - f1_score: 0.7122\n",
      "Epoch 4: val_f1_score improved from 0.70826 to 0.71296, saving model to bilstm_multi_1.h5\n",
      "429/429 [==============================] - 86s 201ms/step - loss: 0.6280 - accuracy: 0.7174 - f1_score: 0.7122 - val_loss: 0.6829 - val_accuracy: 0.7182 - val_f1_score: 0.7130 - lr: 0.0010\n",
      "Epoch 5/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.6129 - accuracy: 0.7223 - f1_score: 0.7165\n",
      "Epoch 5: val_f1_score did not improve from 0.71296\n",
      "429/429 [==============================] - 86s 200ms/step - loss: 0.6129 - accuracy: 0.7223 - f1_score: 0.7165 - val_loss: 0.7040 - val_accuracy: 0.7013 - val_f1_score: 0.6921 - lr: 0.0010\n",
      "Epoch 6/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.5899 - accuracy: 0.7354 - f1_score: 0.7307\n",
      "Epoch 6: val_f1_score improved from 0.71296 to 0.71602, saving model to bilstm_multi_1.h5\n",
      "429/429 [==============================] - 85s 199ms/step - loss: 0.5899 - accuracy: 0.7354 - f1_score: 0.7307 - val_loss: 0.7074 - val_accuracy: 0.7188 - val_f1_score: 0.7160 - lr: 0.0010\n",
      "Epoch 7/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.5581 - accuracy: 0.7431 - f1_score: 0.7415\n",
      "Epoch 7: val_f1_score did not improve from 0.71602\n",
      "429/429 [==============================] - 85s 199ms/step - loss: 0.5581 - accuracy: 0.7431 - f1_score: 0.7415 - val_loss: 0.7608 - val_accuracy: 0.7083 - val_f1_score: 0.7086 - lr: 0.0010\n",
      "Epoch 8/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.5338 - accuracy: 0.7637 - f1_score: 0.7582\n",
      "Epoch 8: val_f1_score did not improve from 0.71602\n",
      "429/429 [==============================] - 85s 199ms/step - loss: 0.5338 - accuracy: 0.7637 - f1_score: 0.7582 - val_loss: 0.7574 - val_accuracy: 0.7060 - val_f1_score: 0.7033 - lr: 0.0010\n",
      "Epoch 9/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.5007 - accuracy: 0.7803 - f1_score: 0.7786\n",
      "Epoch 9: val_f1_score did not improve from 0.71602\n",
      "429/429 [==============================] - 87s 203ms/step - loss: 0.5007 - accuracy: 0.7803 - f1_score: 0.7786 - val_loss: 0.8513 - val_accuracy: 0.6534 - val_f1_score: 0.6449 - lr: 0.0010\n",
      "Epoch 10/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.4592 - accuracy: 0.7999 - f1_score: 0.7986\n",
      "Epoch 10: val_f1_score did not improve from 0.71602\n",
      "429/429 [==============================] - 88s 205ms/step - loss: 0.4592 - accuracy: 0.7999 - f1_score: 0.7986 - val_loss: 0.9269 - val_accuracy: 0.7019 - val_f1_score: 0.7001 - lr: 0.0010\n",
      "Epoch 11/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.4358 - accuracy: 0.8093 - f1_score: 0.8096\n",
      "Epoch 11: val_f1_score did not improve from 0.71602\n",
      "429/429 [==============================] - 88s 206ms/step - loss: 0.4358 - accuracy: 0.8093 - f1_score: 0.8096 - val_loss: 0.9355 - val_accuracy: 0.6995 - val_f1_score: 0.6982 - lr: 0.0010\n",
      "Epoch 1/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.7087 - accuracy: 0.7021 - f1_score: 0.6869\n",
      "Epoch 1: val_f1_score improved from -inf to 0.71173, saving model to bilstm_multi_2.h5\n",
      "429/429 [==============================] - 73s 159ms/step - loss: 0.7087 - accuracy: 0.7021 - f1_score: 0.6869 - val_loss: 0.7100 - val_accuracy: 0.7135 - val_f1_score: 0.7117 - lr: 0.0010\n",
      "Epoch 2/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.6659 - accuracy: 0.7096 - f1_score: 0.6996\n",
      "Epoch 2: val_f1_score did not improve from 0.71173\n",
      "429/429 [==============================] - 67s 157ms/step - loss: 0.6659 - accuracy: 0.7096 - f1_score: 0.6996 - val_loss: 0.7512 - val_accuracy: 0.7095 - val_f1_score: 0.7096 - lr: 0.0010\n",
      "Epoch 3/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.6493 - accuracy: 0.7123 - f1_score: 0.7050\n",
      "Epoch 3: val_f1_score did not improve from 0.71173\n",
      "429/429 [==============================] - 68s 157ms/step - loss: 0.6493 - accuracy: 0.7123 - f1_score: 0.7050 - val_loss: 0.6906 - val_accuracy: 0.7100 - val_f1_score: 0.6958 - lr: 0.0010\n",
      "Epoch 4/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.6280 - accuracy: 0.7213 - f1_score: 0.7142\n",
      "Epoch 4: val_f1_score did not improve from 0.71173\n",
      "429/429 [==============================] - 69s 161ms/step - loss: 0.6280 - accuracy: 0.7213 - f1_score: 0.7142 - val_loss: 0.7103 - val_accuracy: 0.7048 - val_f1_score: 0.7002 - lr: 0.0010\n",
      "Epoch 5/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.6091 - accuracy: 0.7259 - f1_score: 0.7202\n",
      "Epoch 5: val_f1_score did not improve from 0.71173\n",
      "429/429 [==============================] - 68s 159ms/step - loss: 0.6091 - accuracy: 0.7259 - f1_score: 0.7202 - val_loss: 0.6960 - val_accuracy: 0.6978 - val_f1_score: 0.6790 - lr: 0.0010\n",
      "Epoch 6/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.5898 - accuracy: 0.7350 - f1_score: 0.7304\n",
      "Epoch 6: val_f1_score did not improve from 0.71173\n",
      "429/429 [==============================] - 70s 162ms/step - loss: 0.5898 - accuracy: 0.7350 - f1_score: 0.7304 - val_loss: 1.6790 - val_accuracy: 0.3471 - val_f1_score: 0.3422 - lr: 0.0010\n",
      "Epoch 7/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.5758 - accuracy: 0.7418 - f1_score: 0.7370\n",
      "Epoch 7: val_f1_score did not improve from 0.71173\n",
      "429/429 [==============================] - 71s 164ms/step - loss: 0.5758 - accuracy: 0.7418 - f1_score: 0.7370 - val_loss: 0.7340 - val_accuracy: 0.7130 - val_f1_score: 0.7106 - lr: 0.0010\n",
      "Epoch 8/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.5353 - accuracy: 0.7614 - f1_score: 0.7585\n",
      "Epoch 8: val_f1_score did not improve from 0.71173\n",
      "429/429 [==============================] - 71s 165ms/step - loss: 0.5353 - accuracy: 0.7614 - f1_score: 0.7585 - val_loss: 0.7742 - val_accuracy: 0.6978 - val_f1_score: 0.6992 - lr: 0.0010\n",
      "Epoch 9/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.5060 - accuracy: 0.7779 - f1_score: 0.7752\n",
      "Epoch 9: val_f1_score did not improve from 0.71173\n",
      "429/429 [==============================] - 70s 164ms/step - loss: 0.5060 - accuracy: 0.7779 - f1_score: 0.7752 - val_loss: 0.7766 - val_accuracy: 0.6989 - val_f1_score: 0.6976 - lr: 0.0010\n",
      "Epoch 10/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.4743 - accuracy: 0.7931 - f1_score: 0.7894\n",
      "Epoch 10: val_f1_score did not improve from 0.71173\n",
      "429/429 [==============================] - 69s 162ms/step - loss: 0.4743 - accuracy: 0.7931 - f1_score: 0.7894 - val_loss: 0.8439 - val_accuracy: 0.7106 - val_f1_score: 0.7069 - lr: 0.0010\n",
      "Epoch 11/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.4406 - accuracy: 0.8103 - f1_score: 0.8095\n",
      "Epoch 11: val_f1_score did not improve from 0.71173\n",
      "429/429 [==============================] - 70s 164ms/step - loss: 0.4406 - accuracy: 0.8103 - f1_score: 0.8095 - val_loss: 0.8879 - val_accuracy: 0.6978 - val_f1_score: 0.6959 - lr: 0.0010\n",
      "Epoch 12/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.4063 - accuracy: 0.8230 - f1_score: 0.8205\n",
      "Epoch 12: val_f1_score did not improve from 0.71173\n",
      "429/429 [==============================] - 72s 169ms/step - loss: 0.4063 - accuracy: 0.8230 - f1_score: 0.8205 - val_loss: 0.9827 - val_accuracy: 0.6908 - val_f1_score: 0.6891 - lr: 0.0010\n",
      "Epoch 13/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.3679 - accuracy: 0.8425 - f1_score: 0.8422\n",
      "Epoch 13: val_f1_score did not improve from 0.71173\n",
      "429/429 [==============================] - 73s 169ms/step - loss: 0.3679 - accuracy: 0.8425 - f1_score: 0.8422 - val_loss: 0.9786 - val_accuracy: 0.6762 - val_f1_score: 0.6737 - lr: 0.0010\n",
      "Epoch 1/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.7095 - accuracy: 0.7010 - f1_score: 0.6865\n",
      "Epoch 1: val_f1_score improved from -inf to 0.66402, saving model to bilstm_multi_3.h5\n",
      "429/429 [==============================] - 73s 159ms/step - loss: 0.7095 - accuracy: 0.7010 - f1_score: 0.6865 - val_loss: 0.7203 - val_accuracy: 0.6867 - val_f1_score: 0.6640 - lr: 0.0010\n",
      "Epoch 2/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.6673 - accuracy: 0.7086 - f1_score: 0.6991\n",
      "Epoch 2: val_f1_score improved from 0.66402 to 0.69573, saving model to bilstm_multi_3.h5\n",
      "429/429 [==============================] - 68s 158ms/step - loss: 0.6673 - accuracy: 0.7086 - f1_score: 0.6991 - val_loss: 0.6783 - val_accuracy: 0.7106 - val_f1_score: 0.6957 - lr: 0.0010\n",
      "Epoch 3/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.6451 - accuracy: 0.7142 - f1_score: 0.7078\n",
      "Epoch 3: val_f1_score improved from 0.69573 to 0.70049, saving model to bilstm_multi_3.h5\n",
      "429/429 [==============================] - 67s 156ms/step - loss: 0.6451 - accuracy: 0.7142 - f1_score: 0.7078 - val_loss: 0.6774 - val_accuracy: 0.7083 - val_f1_score: 0.7005 - lr: 0.0010\n",
      "Epoch 4/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.6310 - accuracy: 0.7199 - f1_score: 0.7127\n",
      "Epoch 4: val_f1_score improved from 0.70049 to 0.70874, saving model to bilstm_multi_3.h5\n",
      "429/429 [==============================] - 68s 158ms/step - loss: 0.6310 - accuracy: 0.7199 - f1_score: 0.7127 - val_loss: 0.6758 - val_accuracy: 0.7135 - val_f1_score: 0.7087 - lr: 0.0010\n",
      "Epoch 5/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.6114 - accuracy: 0.7270 - f1_score: 0.7213\n",
      "Epoch 5: val_f1_score did not improve from 0.70874\n",
      "429/429 [==============================] - 69s 160ms/step - loss: 0.6114 - accuracy: 0.7270 - f1_score: 0.7213 - val_loss: 0.6936 - val_accuracy: 0.7112 - val_f1_score: 0.7062 - lr: 0.0010\n",
      "Epoch 6/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.5883 - accuracy: 0.7362 - f1_score: 0.7304\n",
      "Epoch 6: val_f1_score did not improve from 0.70874\n",
      "429/429 [==============================] - 67s 155ms/step - loss: 0.5883 - accuracy: 0.7362 - f1_score: 0.7304 - val_loss: 0.7092 - val_accuracy: 0.7089 - val_f1_score: 0.7033 - lr: 0.0010\n",
      "Epoch 7/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.5614 - accuracy: 0.7472 - f1_score: 0.7415\n",
      "Epoch 7: val_f1_score did not improve from 0.70874\n",
      "429/429 [==============================] - 67s 156ms/step - loss: 0.5614 - accuracy: 0.7472 - f1_score: 0.7415 - val_loss: 0.7489 - val_accuracy: 0.7019 - val_f1_score: 0.7000 - lr: 0.0010\n",
      "Epoch 8/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.5322 - accuracy: 0.7653 - f1_score: 0.7611\n",
      "Epoch 8: val_f1_score did not improve from 0.70874\n",
      "429/429 [==============================] - 67s 155ms/step - loss: 0.5322 - accuracy: 0.7653 - f1_score: 0.7611 - val_loss: 0.7074 - val_accuracy: 0.7106 - val_f1_score: 0.7083 - lr: 0.0010\n",
      "Epoch 9/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.4934 - accuracy: 0.7839 - f1_score: 0.7812\n",
      "Epoch 9: val_f1_score did not improve from 0.70874\n",
      "429/429 [==============================] - 66s 155ms/step - loss: 0.4934 - accuracy: 0.7839 - f1_score: 0.7812 - val_loss: 0.7572 - val_accuracy: 0.7060 - val_f1_score: 0.7017 - lr: 0.0010\n",
      "Epoch 10/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.4719 - accuracy: 0.7987 - f1_score: 0.7964\n",
      "Epoch 10: val_f1_score did not improve from 0.70874\n",
      "429/429 [==============================] - 67s 156ms/step - loss: 0.4719 - accuracy: 0.7987 - f1_score: 0.7964 - val_loss: 0.8540 - val_accuracy: 0.6779 - val_f1_score: 0.6737 - lr: 0.0010\n",
      "Epoch 11/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.4339 - accuracy: 0.8128 - f1_score: 0.8111\n",
      "Epoch 11: val_f1_score did not improve from 0.70874\n",
      "429/429 [==============================] - 67s 156ms/step - loss: 0.4339 - accuracy: 0.8128 - f1_score: 0.8111 - val_loss: 0.9283 - val_accuracy: 0.7013 - val_f1_score: 0.7011 - lr: 0.0010\n",
      "Epoch 12/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.4051 - accuracy: 0.8249 - f1_score: 0.8221\n",
      "Epoch 12: val_f1_score did not improve from 0.70874\n",
      "429/429 [==============================] - 67s 156ms/step - loss: 0.4051 - accuracy: 0.8249 - f1_score: 0.8221 - val_loss: 0.9743 - val_accuracy: 0.6628 - val_f1_score: 0.6594 - lr: 0.0010\n",
      "Epoch 13/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.3792 - accuracy: 0.8371 - f1_score: 0.8364\n",
      "Epoch 13: val_f1_score did not improve from 0.70874\n",
      "429/429 [==============================] - 66s 155ms/step - loss: 0.3792 - accuracy: 0.8371 - f1_score: 0.8364 - val_loss: 1.0760 - val_accuracy: 0.6068 - val_f1_score: 0.5997 - lr: 0.0010\n",
      "Epoch 14/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.3529 - accuracy: 0.8508 - f1_score: 0.8509\n",
      "Epoch 14: val_f1_score did not improve from 0.70874\n",
      "429/429 [==============================] - 68s 158ms/step - loss: 0.3529 - accuracy: 0.8508 - f1_score: 0.8509 - val_loss: 1.2330 - val_accuracy: 0.6919 - val_f1_score: 0.6910 - lr: 0.0010\n",
      "Epoch 1/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.7076 - accuracy: 0.7047 - f1_score: 0.6908\n",
      "Epoch 1: val_f1_score improved from -inf to 0.70475, saving model to bilstm_multi_4.h5\n",
      "429/429 [==============================] - 73s 159ms/step - loss: 0.7076 - accuracy: 0.7047 - f1_score: 0.6908 - val_loss: 0.6684 - val_accuracy: 0.7153 - val_f1_score: 0.7047 - lr: 0.0010\n",
      "Epoch 2/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.6656 - accuracy: 0.7123 - f1_score: 0.7030\n",
      "Epoch 2: val_f1_score improved from 0.70475 to 0.71363, saving model to bilstm_multi_4.h5\n",
      "429/429 [==============================] - 66s 155ms/step - loss: 0.6656 - accuracy: 0.7123 - f1_score: 0.7030 - val_loss: 0.6876 - val_accuracy: 0.7165 - val_f1_score: 0.7136 - lr: 0.0010\n",
      "Epoch 3/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.6474 - accuracy: 0.7132 - f1_score: 0.7049\n",
      "Epoch 3: val_f1_score did not improve from 0.71363\n",
      "429/429 [==============================] - 66s 154ms/step - loss: 0.6474 - accuracy: 0.7132 - f1_score: 0.7049 - val_loss: 0.6828 - val_accuracy: 0.7141 - val_f1_score: 0.6884 - lr: 0.0010\n",
      "Epoch 4/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.6255 - accuracy: 0.7207 - f1_score: 0.7143\n",
      "Epoch 4: val_f1_score did not improve from 0.71363\n",
      "429/429 [==============================] - 66s 154ms/step - loss: 0.6255 - accuracy: 0.7207 - f1_score: 0.7143 - val_loss: 0.6842 - val_accuracy: 0.7112 - val_f1_score: 0.7048 - lr: 0.0010\n",
      "Epoch 5/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.6099 - accuracy: 0.7263 - f1_score: 0.7193\n",
      "Epoch 5: val_f1_score did not improve from 0.71363\n",
      "429/429 [==============================] - 67s 155ms/step - loss: 0.6099 - accuracy: 0.7263 - f1_score: 0.7193 - val_loss: 0.6845 - val_accuracy: 0.7118 - val_f1_score: 0.6919 - lr: 0.0010\n",
      "Epoch 6/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.5897 - accuracy: 0.7326 - f1_score: 0.7289\n",
      "Epoch 6: val_f1_score did not improve from 0.71363\n",
      "429/429 [==============================] - 66s 155ms/step - loss: 0.5897 - accuracy: 0.7326 - f1_score: 0.7289 - val_loss: 0.7275 - val_accuracy: 0.7141 - val_f1_score: 0.7135 - lr: 0.0010\n",
      "Epoch 7/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.5613 - accuracy: 0.7491 - f1_score: 0.7444\n",
      "Epoch 7: val_f1_score did not improve from 0.71363\n",
      "429/429 [==============================] - 67s 155ms/step - loss: 0.5613 - accuracy: 0.7491 - f1_score: 0.7444 - val_loss: 0.7093 - val_accuracy: 0.7089 - val_f1_score: 0.7053 - lr: 0.0010\n",
      "Epoch 8/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.5331 - accuracy: 0.7624 - f1_score: 0.7601\n",
      "Epoch 8: val_f1_score did not improve from 0.71363\n",
      "429/429 [==============================] - 67s 156ms/step - loss: 0.5331 - accuracy: 0.7624 - f1_score: 0.7601 - val_loss: 0.7182 - val_accuracy: 0.7042 - val_f1_score: 0.6952 - lr: 0.0010\n",
      "Epoch 9/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.5009 - accuracy: 0.7786 - f1_score: 0.7769\n",
      "Epoch 9: val_f1_score did not improve from 0.71363\n",
      "429/429 [==============================] - 66s 153ms/step - loss: 0.5009 - accuracy: 0.7786 - f1_score: 0.7769 - val_loss: 0.7586 - val_accuracy: 0.6966 - val_f1_score: 0.6928 - lr: 0.0010\n",
      "Epoch 10/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.4705 - accuracy: 0.7959 - f1_score: 0.7948\n",
      "Epoch 10: val_f1_score did not improve from 0.71363\n",
      "429/429 [==============================] - 68s 158ms/step - loss: 0.4705 - accuracy: 0.7959 - f1_score: 0.7948 - val_loss: 0.7801 - val_accuracy: 0.7089 - val_f1_score: 0.7065 - lr: 0.0010\n",
      "Epoch 11/30\n",
      "429/429 [==============================] - ETA: 0s - loss: 0.4343 - accuracy: 0.8130 - f1_score: 0.8102\n",
      "Epoch 11: val_f1_score did not improve from 0.71363\n",
      "429/429 [==============================] - 67s 156ms/step - loss: 0.4343 - accuracy: 0.8130 - f1_score: 0.8102 - val_loss: 4.0598 - val_accuracy: 0.3582 - val_f1_score: 0.3587 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "num_models = 5 # number of models in the ensemble\n",
    "\n",
    "models = [] # list to hold the models\n",
    "histories = [] # list to hold the training histories of the models\n",
    "\n",
    "for i in range(num_models):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(num_words, embedding_dim, weights=[embedding_matrix], input_length=100, trainable=False))\n",
    "    model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Bidirectional(LSTM(64)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    model.compile(loss=CategoricalCrossentropy(), optimizer=RMSprop(learning_rate=0.001), metrics=['accuracy', f1_score])\n",
    "\n",
    "    checkpoint = ModelCheckpoint('bilstm_multi_{}.h5'.format(i), monitor='val_f1_score', verbose=1, save_best_only=True, mode='max')\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "    history = model.fit(X_train_padded, Y_train_encoded, epochs=30, batch_size=32, validation_data=(X_val_padded, Y_val_encoded), callbacks=[checkpoint, reduce_lr, early_stop])\n",
    "\n",
    "    best_model = load_model('bilstm_multi_{}.h5'.format(i), custom_objects={'f1_score': f1_score})\n",
    "    models.append(model)\n",
    "    histories.append(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 2s 85ms/step - loss: 1.0550 - accuracy: 0.6795 - f1_score: 0.6776\n",
      "Test loss:  1.0549625158309937\n",
      "Test accuracy:  0.6795096397399902\n",
      "Test f1 score:  0.6776270866394043\n",
      "27/27 [==============================] - 2s 83ms/step - loss: 0.8914 - accuracy: 0.6953 - f1_score: 0.6950\n",
      "Test loss:  0.891429603099823\n",
      "Test accuracy:  0.6952714323997498\n",
      "Test f1 score:  0.6949810981750488\n",
      "27/27 [==============================] - 2s 83ms/step - loss: 1.0061 - accuracy: 0.6824 - f1_score: 0.6808\n",
      "Test loss:  1.006123423576355\n",
      "Test accuracy:  0.6824284791946411\n",
      "Test f1 score:  0.680793285369873\n",
      "27/27 [==============================] - 2s 84ms/step - loss: 1.1994 - accuracy: 0.6988 - f1_score: 0.6993\n",
      "Test loss:  1.1993995904922485\n",
      "Test accuracy:  0.6987740993499756\n",
      "Test f1 score:  0.6993421912193298\n",
      "27/27 [==============================] - 2s 87ms/step - loss: 3.8809 - accuracy: 0.3765 - f1_score: 0.3758\n",
      "Test loss:  3.8808786869049072\n",
      "Test accuracy:  0.3765324056148529\n",
      "Test f1 score:  0.37577807903289795\n"
     ]
    }
   ],
   "source": [
    "for mod in models:\n",
    "    test_loss, test_acc, test_f1 = mod.evaluate(X_test_padded, Y_test_encoded, batch_size=64)\n",
    "    print('Test loss: ', test_loss)\n",
    "    print('Test accuracy: ', test_acc)\n",
    "    print('Test f1 score: ', test_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54/54 [==============================] - 3s 50ms/step\n",
      "54/54 [==============================] - 3s 49ms/step\n",
      "54/54 [==============================] - 3s 48ms/step\n",
      "54/54 [==============================] - 3s 51ms/step\n",
      "54/54 [==============================] - 3s 47ms/step\n",
      "54/54 [==============================] - 3s 49ms/step\n",
      "54/54 [==============================] - 3s 49ms/step\n",
      "54/54 [==============================] - 3s 50ms/step\n",
      "54/54 [==============================] - 3s 49ms/step\n",
      "54/54 [==============================] - 3s 51ms/step\n",
      "F1 score:  0.36214364635367985\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Get the prediction of each model on the validation data\n",
    "predictions = []\n",
    "for model in models:\n",
    "    pred = model.predict(X_val_padded)\n",
    "    predictions.append(pred)\n",
    "\n",
    "# Stack predictions horizontally (each model's predictions are a column)\n",
    "predictions = np.hstack(predictions)\n",
    "\n",
    "# Train a logistic regression model on the stacked predictions\n",
    "meta_model = LogisticRegression()\n",
    "meta_model.fit(predictions, Y_val_encoded.argmax(axis=1))\n",
    "\n",
    "# Get the predictions of each model on the test data\n",
    "predictions_test = []\n",
    "for model in models:\n",
    "    pred = model.predict(X_test_padded)\n",
    "    predictions_test.append(pred)\n",
    "\n",
    "# Stack predictions horizontally (each model's predictions are a column)\n",
    "predictions_test = np.hstack(predictions_test)\n",
    "\n",
    "# Predict the classes on the test data using the meta-model\n",
    "y_pred = meta_model.predict(predictions_test)\n",
    "\n",
    "# Calculate and print the F1 score\n",
    "macro_f1_score = f1_score(Y_test_encoded.argmax(axis=1), y_pred, average='macro')\n",
    "print('F1 score: ', macro_f1_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "429/429 [==============================] - 69s 149ms/step - loss: 0.7173 - accuracy: 0.7030 - val_loss: 0.6847 - val_accuracy: 0.7135 - lr: 0.0010\n",
      "Epoch 2/30\n",
      "429/429 [==============================] - 65s 152ms/step - loss: 0.6692 - accuracy: 0.7092 - val_loss: 0.6795 - val_accuracy: 0.7159 - lr: 0.0010\n",
      "Epoch 3/30\n",
      "429/429 [==============================] - 65s 153ms/step - loss: 0.6483 - accuracy: 0.7126 - val_loss: 0.6819 - val_accuracy: 0.7159 - lr: 0.0010\n",
      "Epoch 4/30\n",
      "429/429 [==============================] - 71s 165ms/step - loss: 0.6307 - accuracy: 0.7165 - val_loss: 0.6890 - val_accuracy: 0.6937 - lr: 0.0010\n",
      "Epoch 5/30\n",
      "429/429 [==============================] - 68s 158ms/step - loss: 0.6115 - accuracy: 0.7267 - val_loss: 0.7051 - val_accuracy: 0.7153 - lr: 0.0010\n",
      "Epoch 6/30\n",
      "429/429 [==============================] - 66s 155ms/step - loss: 0.5912 - accuracy: 0.7347 - val_loss: 0.7166 - val_accuracy: 0.6919 - lr: 0.0010\n",
      "Epoch 7/30\n",
      "429/429 [==============================] - 67s 157ms/step - loss: 0.5667 - accuracy: 0.7475 - val_loss: 0.7110 - val_accuracy: 0.7083 - lr: 0.0010\n",
      "Epoch 8/30\n",
      "429/429 [==============================] - 70s 163ms/step - loss: 0.5359 - accuracy: 0.7642 - val_loss: 0.7177 - val_accuracy: 0.7083 - lr: 0.0010\n",
      "Epoch 9/30\n",
      "429/429 [==============================] - 68s 160ms/step - loss: 0.5154 - accuracy: 0.7739 - val_loss: 0.7548 - val_accuracy: 0.6849 - lr: 0.0010\n",
      "Epoch 10/30\n",
      "429/429 [==============================] - 69s 161ms/step - loss: 0.4755 - accuracy: 0.7931 - val_loss: 0.8130 - val_accuracy: 0.7019 - lr: 0.0010\n",
      "Epoch 11/30\n",
      "429/429 [==============================] - 68s 159ms/step - loss: 0.4398 - accuracy: 0.8087 - val_loss: 0.8331 - val_accuracy: 0.6943 - lr: 0.0010\n",
      "Epoch 12/30\n",
      "429/429 [==============================] - 68s 159ms/step - loss: 0.4036 - accuracy: 0.8279 - val_loss: 0.9442 - val_accuracy: 0.7060 - lr: 0.0010\n",
      "429/429 [==============================] - 25s 56ms/step\n",
      "Epoch 1/30\n",
      "429/429 [==============================] - 73s 160ms/step - loss: 0.7154 - accuracy: 0.7021 - val_loss: 0.7067 - val_accuracy: 0.7071 - lr: 0.0010\n",
      "Epoch 2/30\n",
      "429/429 [==============================] - 68s 158ms/step - loss: 0.6656 - accuracy: 0.7107 - val_loss: 0.6863 - val_accuracy: 0.6896 - lr: 0.0010\n",
      "Epoch 3/30\n",
      "429/429 [==============================] - 67s 155ms/step - loss: 0.6495 - accuracy: 0.7144 - val_loss: 0.6800 - val_accuracy: 0.7141 - lr: 0.0010\n",
      "Epoch 4/30\n",
      "429/429 [==============================] - 67s 155ms/step - loss: 0.6299 - accuracy: 0.7208 - val_loss: 0.6724 - val_accuracy: 0.7112 - lr: 0.0010\n",
      "Epoch 5/30\n",
      "429/429 [==============================] - 67s 157ms/step - loss: 0.6102 - accuracy: 0.7279 - val_loss: 0.6908 - val_accuracy: 0.7025 - lr: 0.0010\n",
      "Epoch 6/30\n",
      "429/429 [==============================] - 69s 161ms/step - loss: 0.5913 - accuracy: 0.7345 - val_loss: 0.7316 - val_accuracy: 0.7165 - lr: 0.0010\n",
      "Epoch 7/30\n",
      "429/429 [==============================] - 68s 159ms/step - loss: 0.5695 - accuracy: 0.7450 - val_loss: 0.7223 - val_accuracy: 0.7083 - lr: 0.0010\n",
      "Epoch 8/30\n",
      "429/429 [==============================] - 68s 159ms/step - loss: 0.5400 - accuracy: 0.7596 - val_loss: 0.7307 - val_accuracy: 0.7112 - lr: 0.0010\n",
      "Epoch 9/30\n",
      "429/429 [==============================] - 68s 159ms/step - loss: 0.5037 - accuracy: 0.7772 - val_loss: 0.8160 - val_accuracy: 0.6750 - lr: 0.0010\n",
      "Epoch 10/30\n",
      "429/429 [==============================] - 68s 159ms/step - loss: 0.4747 - accuracy: 0.7890 - val_loss: 0.8629 - val_accuracy: 0.6861 - lr: 0.0010\n",
      "Epoch 11/30\n",
      "429/429 [==============================] - 68s 159ms/step - loss: 0.4397 - accuracy: 0.8085 - val_loss: 0.8652 - val_accuracy: 0.6989 - lr: 0.0010\n",
      "Epoch 12/30\n",
      "429/429 [==============================] - 67s 157ms/step - loss: 0.4028 - accuracy: 0.8241 - val_loss: 1.0112 - val_accuracy: 0.6931 - lr: 0.0010\n",
      "Epoch 13/30\n",
      "429/429 [==============================] - 67s 157ms/step - loss: 0.3977 - accuracy: 0.8317 - val_loss: 0.9530 - val_accuracy: 0.6593 - lr: 0.0010\n",
      "Epoch 14/30\n",
      "429/429 [==============================] - 68s 158ms/step - loss: 0.3499 - accuracy: 0.8454 - val_loss: 1.1968 - val_accuracy: 0.6447 - lr: 0.0010\n",
      "429/429 [==============================] - 25s 57ms/step\n",
      "Epoch 1/30\n",
      "429/429 [==============================] - 75s 164ms/step - loss: 0.7097 - accuracy: 0.7038 - val_loss: 0.6734 - val_accuracy: 0.7124 - lr: 0.0010\n",
      "Epoch 2/30\n",
      "429/429 [==============================] - 68s 160ms/step - loss: 0.6659 - accuracy: 0.7076 - val_loss: 0.6761 - val_accuracy: 0.7153 - lr: 0.0010\n",
      "Epoch 3/30\n",
      "429/429 [==============================] - 68s 159ms/step - loss: 0.6464 - accuracy: 0.7120 - val_loss: 0.6672 - val_accuracy: 0.7159 - lr: 0.0010\n",
      "Epoch 4/30\n",
      "429/429 [==============================] - 69s 160ms/step - loss: 0.6279 - accuracy: 0.7173 - val_loss: 0.6642 - val_accuracy: 0.7124 - lr: 0.0010\n",
      "Epoch 5/30\n",
      "429/429 [==============================] - 68s 160ms/step - loss: 0.6103 - accuracy: 0.7260 - val_loss: 0.6774 - val_accuracy: 0.7159 - lr: 0.0010\n",
      "Epoch 6/30\n",
      "429/429 [==============================] - 73s 169ms/step - loss: 0.5914 - accuracy: 0.7342 - val_loss: 0.6974 - val_accuracy: 0.7106 - lr: 0.0010\n",
      "Epoch 7/30\n",
      "429/429 [==============================] - 73s 170ms/step - loss: 0.5668 - accuracy: 0.7499 - val_loss: 0.7412 - val_accuracy: 0.6698 - lr: 0.0010\n",
      "Epoch 8/30\n",
      "429/429 [==============================] - 72s 167ms/step - loss: 0.5497 - accuracy: 0.7592 - val_loss: 0.8296 - val_accuracy: 0.7048 - lr: 0.0010\n",
      "Epoch 9/30\n",
      "429/429 [==============================] - 69s 161ms/step - loss: 0.5139 - accuracy: 0.7752 - val_loss: 0.8476 - val_accuracy: 0.7095 - lr: 0.0010\n",
      "Epoch 10/30\n",
      "429/429 [==============================] - 69s 161ms/step - loss: 0.4769 - accuracy: 0.7925 - val_loss: 0.8372 - val_accuracy: 0.6884 - lr: 0.0010\n",
      "Epoch 11/30\n",
      "429/429 [==============================] - 67s 157ms/step - loss: 0.4342 - accuracy: 0.8125 - val_loss: 0.8457 - val_accuracy: 0.6972 - lr: 0.0010\n",
      "Epoch 12/30\n",
      "429/429 [==============================] - 68s 159ms/step - loss: 0.4109 - accuracy: 0.8240 - val_loss: 0.8965 - val_accuracy: 0.6995 - lr: 0.0010\n",
      "Epoch 13/30\n",
      "429/429 [==============================] - 67s 157ms/step - loss: 0.4051 - accuracy: 0.8340 - val_loss: 0.9526 - val_accuracy: 0.6890 - lr: 0.0010\n",
      "Epoch 14/30\n",
      "429/429 [==============================] - 68s 157ms/step - loss: 0.3581 - accuracy: 0.8495 - val_loss: 1.0905 - val_accuracy: 0.6144 - lr: 0.0010\n",
      "429/429 [==============================] - 26s 58ms/step\n",
      "Epoch 1/30\n",
      "429/429 [==============================] - 74s 162ms/step - loss: 0.7131 - accuracy: 0.7052 - val_loss: 0.6856 - val_accuracy: 0.7095 - lr: 0.0010\n",
      "Epoch 2/30\n",
      "429/429 [==============================] - 69s 162ms/step - loss: 0.6672 - accuracy: 0.7102 - val_loss: 0.6969 - val_accuracy: 0.7130 - lr: 0.0010\n",
      "Epoch 3/30\n",
      "429/429 [==============================] - 67s 157ms/step - loss: 0.6496 - accuracy: 0.7126 - val_loss: 0.6699 - val_accuracy: 0.7147 - lr: 0.0010\n",
      "Epoch 4/30\n",
      "429/429 [==============================] - 70s 164ms/step - loss: 0.6320 - accuracy: 0.7173 - val_loss: 0.7047 - val_accuracy: 0.7124 - lr: 0.0010\n",
      "Epoch 5/30\n",
      "429/429 [==============================] - 86s 199ms/step - loss: 0.6118 - accuracy: 0.7253 - val_loss: 0.6852 - val_accuracy: 0.7135 - lr: 0.0010\n",
      "Epoch 6/30\n",
      "429/429 [==============================] - 82s 192ms/step - loss: 0.5867 - accuracy: 0.7366 - val_loss: 0.7529 - val_accuracy: 0.7130 - lr: 0.0010\n",
      "Epoch 7/30\n",
      "429/429 [==============================] - 82s 191ms/step - loss: 0.5651 - accuracy: 0.7439 - val_loss: 0.7471 - val_accuracy: 0.7095 - lr: 0.0010\n",
      "Epoch 8/30\n",
      "429/429 [==============================] - 89s 208ms/step - loss: 0.5322 - accuracy: 0.7625 - val_loss: 0.7681 - val_accuracy: 0.6826 - lr: 0.0010\n",
      "Epoch 9/30\n",
      "429/429 [==============================] - 87s 203ms/step - loss: 0.4992 - accuracy: 0.7796 - val_loss: 0.8045 - val_accuracy: 0.6721 - lr: 0.0010\n",
      "Epoch 10/30\n",
      "429/429 [==============================] - 83s 194ms/step - loss: 0.4665 - accuracy: 0.7934 - val_loss: 0.9213 - val_accuracy: 0.6914 - lr: 0.0010\n",
      "Epoch 11/30\n",
      "429/429 [==============================] - 84s 196ms/step - loss: 0.4378 - accuracy: 0.8080 - val_loss: 0.9541 - val_accuracy: 0.6768 - lr: 0.0010\n",
      "Epoch 12/30\n",
      "429/429 [==============================] - 76s 178ms/step - loss: 0.4018 - accuracy: 0.8253 - val_loss: 0.9221 - val_accuracy: 0.7001 - lr: 0.0010\n",
      "Epoch 13/30\n",
      "429/429 [==============================] - 74s 173ms/step - loss: 0.3683 - accuracy: 0.8397 - val_loss: 1.0235 - val_accuracy: 0.6499 - lr: 0.0010\n",
      "429/429 [==============================] - 29s 63ms/step\n",
      "Epoch 1/30\n",
      "429/429 [==============================] - 82s 174ms/step - loss: 0.7145 - accuracy: 0.7058 - val_loss: 0.6974 - val_accuracy: 0.7095 - lr: 0.0010\n",
      "Epoch 2/30\n",
      "429/429 [==============================] - 76s 178ms/step - loss: 0.6725 - accuracy: 0.7091 - val_loss: 0.6805 - val_accuracy: 0.7118 - lr: 0.0010\n",
      "Epoch 3/30\n",
      "429/429 [==============================] - 77s 180ms/step - loss: 0.6525 - accuracy: 0.7123 - val_loss: 0.7229 - val_accuracy: 0.7165 - lr: 0.0010\n",
      "Epoch 4/30\n",
      "429/429 [==============================] - 74s 172ms/step - loss: 0.6334 - accuracy: 0.7150 - val_loss: 0.7274 - val_accuracy: 0.7170 - lr: 0.0010\n",
      "Epoch 5/30\n",
      "429/429 [==============================] - 74s 173ms/step - loss: 0.6118 - accuracy: 0.7183 - val_loss: 0.7391 - val_accuracy: 0.7159 - lr: 0.0010\n",
      "Epoch 6/30\n",
      "429/429 [==============================] - 75s 174ms/step - loss: 0.5997 - accuracy: 0.7277 - val_loss: 0.6895 - val_accuracy: 0.7159 - lr: 0.0010\n",
      "Epoch 7/30\n",
      "429/429 [==============================] - 74s 172ms/step - loss: 0.5730 - accuracy: 0.7398 - val_loss: 0.7612 - val_accuracy: 0.6610 - lr: 0.0010\n",
      "Epoch 8/30\n",
      "429/429 [==============================] - 74s 174ms/step - loss: 0.5436 - accuracy: 0.7542 - val_loss: 0.7482 - val_accuracy: 0.7060 - lr: 0.0010\n",
      "Epoch 9/30\n",
      "429/429 [==============================] - 76s 176ms/step - loss: 0.5080 - accuracy: 0.7723 - val_loss: 0.8201 - val_accuracy: 0.7019 - lr: 0.0010\n",
      "Epoch 10/30\n",
      "429/429 [==============================] - 74s 173ms/step - loss: 0.4765 - accuracy: 0.7901 - val_loss: 0.9876 - val_accuracy: 0.7130 - lr: 0.0010\n",
      "Epoch 11/30\n",
      "429/429 [==============================] - 97s 226ms/step - loss: 0.4410 - accuracy: 0.8060 - val_loss: 0.9952 - val_accuracy: 0.6680 - lr: 0.0010\n",
      "Epoch 12/30\n",
      "429/429 [==============================] - 79s 185ms/step - loss: 0.4076 - accuracy: 0.8213 - val_loss: 1.0044 - val_accuracy: 0.6791 - lr: 0.0010\n",
      "429/429 [==============================] - 29s 64ms/step\n",
      "Epoch 1/30\n",
      "429/429 [==============================] - 1s 2ms/step - loss: 0.2938 - accuracy: 0.8982\n",
      "Epoch 2/30\n",
      "429/429 [==============================] - 1s 2ms/step - loss: 0.2273 - accuracy: 0.9058\n",
      "Epoch 3/30\n",
      "429/429 [==============================] - 1s 2ms/step - loss: 0.2253 - accuracy: 0.9060\n",
      "Epoch 4/30\n",
      "429/429 [==============================] - 1s 2ms/step - loss: 0.2238 - accuracy: 0.9051\n",
      "Epoch 5/30\n",
      "429/429 [==============================] - 1s 2ms/step - loss: 0.2231 - accuracy: 0.9059\n",
      "Epoch 6/30\n",
      "429/429 [==============================] - 1s 2ms/step - loss: 0.2224 - accuracy: 0.9059\n",
      "Epoch 7/30\n",
      "429/429 [==============================] - 1s 2ms/step - loss: 0.2215 - accuracy: 0.9067\n",
      "Epoch 8/30\n",
      "429/429 [==============================] - 1s 2ms/step - loss: 0.2213 - accuracy: 0.9054\n",
      "Epoch 9/30\n",
      "429/429 [==============================] - 1s 2ms/step - loss: 0.2200 - accuracy: 0.9070\n",
      "Epoch 10/30\n",
      "429/429 [==============================] - 1s 2ms/step - loss: 0.2205 - accuracy: 0.9065\n",
      "Epoch 11/30\n",
      "429/429 [==============================] - 1s 2ms/step - loss: 0.2200 - accuracy: 0.9060\n",
      "Epoch 12/30\n",
      "429/429 [==============================] - 1s 2ms/step - loss: 0.2197 - accuracy: 0.9065\n",
      "Epoch 13/30\n",
      "429/429 [==============================] - 1s 2ms/step - loss: 0.2196 - accuracy: 0.9065\n",
      "Epoch 14/30\n",
      "429/429 [==============================] - 1s 2ms/step - loss: 0.2190 - accuracy: 0.9064\n",
      "Epoch 15/30\n",
      "429/429 [==============================] - 1s 2ms/step - loss: 0.2192 - accuracy: 0.9048\n",
      "Epoch 16/30\n",
      "429/429 [==============================] - 1s 2ms/step - loss: 0.2192 - accuracy: 0.9058\n",
      "Epoch 17/30\n",
      "429/429 [==============================] - 1s 2ms/step - loss: 0.2187 - accuracy: 0.9054\n",
      "Epoch 18/30\n",
      "429/429 [==============================] - 1s 2ms/step - loss: 0.2186 - accuracy: 0.9059\n",
      "Epoch 19/30\n",
      "429/429 [==============================] - 1s 2ms/step - loss: 0.2187 - accuracy: 0.9054\n",
      "Epoch 20/30\n",
      "429/429 [==============================] - 1s 1ms/step - loss: 0.2176 - accuracy: 0.9062\n",
      "Epoch 21/30\n",
      "429/429 [==============================] - 1s 2ms/step - loss: 0.2180 - accuracy: 0.9062\n",
      "Epoch 22/30\n",
      "429/429 [==============================] - 1s 2ms/step - loss: 0.2176 - accuracy: 0.9058\n",
      "Epoch 23/30\n",
      "429/429 [==============================] - 1s 2ms/step - loss: 0.2177 - accuracy: 0.9060\n",
      "Epoch 24/30\n",
      "429/429 [==============================] - 1s 1ms/step - loss: 0.2171 - accuracy: 0.9070\n",
      "Epoch 25/30\n",
      "429/429 [==============================] - 1s 1ms/step - loss: 0.2174 - accuracy: 0.9071\n",
      "Epoch 26/30\n",
      "429/429 [==============================] - 1s 1ms/step - loss: 0.2168 - accuracy: 0.9061\n",
      "Epoch 27/30\n",
      "429/429 [==============================] - 1s 2ms/step - loss: 0.2166 - accuracy: 0.9064\n",
      "Epoch 28/30\n",
      "429/429 [==============================] - 1s 1ms/step - loss: 0.2173 - accuracy: 0.9075\n",
      "Epoch 29/30\n",
      "429/429 [==============================] - 0s 1ms/step - loss: 0.2164 - accuracy: 0.9073\n",
      "Epoch 30/30\n",
      "429/429 [==============================] - 1s 1ms/step - loss: 0.2165 - accuracy: 0.9064\n",
      "54/54 [==============================] - 3s 59ms/step\n",
      "54/54 [==============================] - 3s 55ms/step\n",
      "54/54 [==============================] - 3s 63ms/step\n",
      "54/54 [==============================] - 3s 60ms/step\n",
      "54/54 [==============================] - 3s 58ms/step\n",
      "54/54 [==============================] - 0s 988us/step\n",
      "F1 score:  0.4269452595363923\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout, Input\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "import numpy as np\n",
    "\n",
    "num_models = 5  # number of models in the ensemble\n",
    "\n",
    "# Function to create and train a BiLSTM model\n",
    "def create_and_train_bilstm_model():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(num_words, embedding_dim, weights=[embedding_matrix], input_length=100, trainable=False))\n",
    "    model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Bidirectional(LSTM(64)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    model.compile(loss=CategoricalCrossentropy(), optimizer=RMSprop(learning_rate=0.001), metrics=['accuracy'])\n",
    "\n",
    "    # checkpoint = ModelCheckpoint('bilstm_multi.h5', monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "    model.fit(X_train_padded, Y_train_encoded, epochs=30, batch_size=32, validation_data=(X_val_padded, Y_val_encoded), callbacks=[reduce_lr, early_stop])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Train multiple BiLSTM models and save their predictions\n",
    "models = []\n",
    "train_predictions = []\n",
    "for i in range(num_models):\n",
    "    model = create_and_train_bilstm_model()\n",
    "    models.append(model)\n",
    "    pred = model.predict(X_train_padded)\n",
    "    train_predictions.append(pred)\n",
    "\n",
    "# Stack predictions horizontally (each model's predictions are a column)\n",
    "train_predictions = np.hstack(train_predictions)\n",
    "\n",
    "# Define the architecture of the meta-model\n",
    "inputs = Input(shape=(train_predictions.shape[1],))\n",
    "x = Dense(64, activation='relu')(inputs)\n",
    "outputs = Dense(3, activation='softmax')(x)\n",
    "meta_model = Model(inputs, outputs)\n",
    "\n",
    "# Compile the meta-model\n",
    "meta_model.compile(optimizer=RMSprop(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the meta-model on the training predictions and the true labels\n",
    "meta_model.fit(train_predictions, Y_train_encoded, epochs=30, batch_size=32)\n",
    "\n",
    "# Get the predictions of each model on the test data\n",
    "test_predictions = []\n",
    "for model in models:\n",
    "    pred = model.predict(X_test_padded)\n",
    "    test_predictions.append(pred)\n",
    "\n",
    "# Stack predictions horizontally (each model's predictions are a column)\n",
    "test_predictions = np.hstack(test_predictions)\n",
    "\n",
    "# Predict the classes on the test data using the meta-model\n",
    "y_pred = np.argmax(meta_model.predict(test_predictions), axis=-1)\n",
    "\n",
    "# Calculate and print the F1 score\n",
    "macro_f1_score = f1_score(Y_test_encoded.argmax(axis=1), y_pred, average='macro')\n",
    "print('F1 score: ', macro_f1_score)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MGNNS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
